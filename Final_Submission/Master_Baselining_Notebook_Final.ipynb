{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASTER NOTEBOOK - EXPLORATORY ANALYSIS & BASELINE METRICS\n",
    "## Final Project - Word Sense Disambiguation\n",
    "### Chris Caudill, Stephanie Fan, Kent Owen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET RELEVANT DATA FROM \"KEY\" FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"ADDED TRAIN/TEST 8-12-17\"\"\"\n",
    "\n",
    "#GET KEYS - UNCOMMENT TO GET RELEVANT DATASET\n",
    "\n",
    "#SEMCOR ONLY\n",
    "with open (\"WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.gold.key_orig.txt\") as f :\n",
    "    semcor_keys = f.readlines()\n",
    "\n",
    "#SEMCOR+OMSTI - COMMENT OUT IF DON'T NEED (TAKES A LONG TIME)\n",
    "# with open (\"WSD_Evaluation_Framework/Training_Corpora/SemCor+OMSTI/semcor+omsti.gold.key.txt\") as f :\n",
    "#     semcor_omsti_keys = f.readlines()\n",
    "\n",
    "#EVAL SETS\n",
    "with open (\"WSD_Evaluation_Framework/Evaluation_Datasets/semeval2007/semeval2007.gold.key.txt\") as f :\n",
    "    semeval2007_keys = f.readlines()\n",
    "with open (\"WSD_Evaluation_Framework/Evaluation_Datasets/semeval2013/semeval2013.gold.key.txt\") as f :\n",
    "    semeval2013_keys = f.readlines()\n",
    "with open (\"WSD_Evaluation_Framework/Evaluation_Datasets/semeval2015/semeval2015.gold.key.txt\") as f :\n",
    "    semeval2015_keys = f.readlines()\n",
    "with open (\"WSD_Evaluation_Framework/Evaluation_Datasets/senseval2/senseval2.gold.key.txt\") as f :\n",
    "    senseval2_keys = f.readlines()\n",
    "with open (\"WSD_Evaluation_Framework/Evaluation_Datasets/senseval3/senseval3.gold.key.txt\") as f :\n",
    "    senseval3_keys = f.readlines()\n",
    "    \n",
    "#SEMCOR TRAIN\n",
    "## COMMENT OUT IF NOT ALREADY CREATED\n",
    "with open (\"WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.gold.key.train.txt\") as f :\n",
    "    semcortrain_keys = f.readlines()\n",
    "\n",
    "with open (\"WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.gold.key.test.txt\") as f :\n",
    "    semcortest_keys = f.readlines()\n",
    "\n",
    "\n",
    "print \"KEYS EXAMPLES\"\n",
    "print semcor_keys[0:10]\n",
    "print semeval2007_keys[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT KEYS DATA INTO TRAIN/TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"KO MODIFIED 8/12/17 - CREATE TRAIN/TEST KEYS AND GET RID OF 2nd WORD SENSE ASSIGNMENT\"\"\"\n",
    "\n",
    "keys_input = semcor_keys\n",
    "output_folder = \"WSD_Evaluation_Framework/Training_Corpora/SemCor/\"\n",
    "train_name = \"semcor.gold.key.train.txt\"\n",
    "test_name = \"semcor.gold.key.test.txt\"\n",
    "train_path = output_folder+train_name\n",
    "test_path = output_folder+test_name\n",
    "\n",
    "with open(train_path,\"w\") as f :\n",
    "    with open (test_path, \"w\") as f2 :\n",
    "        for line in keys_input :\n",
    "            line = line.strip('\\n')\n",
    "            ws_count = line.count(' ')    \n",
    "            #delete 2nd+ word sense assignment(s)\n",
    "            if ws_count > 1 :\n",
    "                index_1 = line.find(' ')\n",
    "                index_2 = (line[index_1+1:].find(' ')+1)+index_1\n",
    "                line = line[:index_2]            \n",
    "            instance = line.split(\".\",2)\n",
    "            article = instance[0]\n",
    "            sentence = instance[1]\n",
    "            tag = instance[2]\n",
    "            article_num = int(article[1:])\n",
    "            if article_num %100 <> 0 :\n",
    "                f.write(line+'\\n')\n",
    "            else :\n",
    "                f2.write(line+'\\n')\n",
    "\n",
    "with open (train_path, \"r\") as f :\n",
    "    train_keys = f.readlines()\n",
    "    \n",
    "    \n",
    "with open (test_path, \"r\") as f2 :\n",
    "    test_keys = f2.readlines()\n",
    "    \n",
    "print train_keys[0:10]\n",
    "print len(train_keys)\n",
    "print len(test_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEAN UP EVAL SET KEY FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 8/22 CC: Added function to take less space in notebook\n",
    "\n",
    "def cleanupKeyFiles(data):\n",
    "\n",
    "    if data=='SemCor':\n",
    "        keys_input = semcor_keys\n",
    "        output_folder = \"WSD_Evaluation_Framework/Training_Corpora/SemCor/\"\n",
    "        orig_name = \"semcor.gold.key_orig.txt\"\n",
    "        clean_name = \"semcor.gold.key.txt\"\n",
    "\n",
    "    elif data=='SemEval2007':\n",
    "        keys_input = semeval2007_keys\n",
    "        output_folder = \"WSD_Evaluation_Framework/Evaluation_Datasets/semeval2007/\"\n",
    "        orig_name = \"semeval2007.gold.key_orig.txt\"\n",
    "        clean_name = \"semeval2007.gold.key.txt\"\n",
    "\n",
    "    elif data=='SemEval2013':\n",
    "        keys_input = semeval2013_keys\n",
    "        output_folder = \"WSD_Evaluation_Framework/Evaluation_Datasets/semeval2013/\"\n",
    "        orig_name = \"semeval2013.gold.key_orig.txt\"\n",
    "        clean_name = \"semeval2013.gold.key.txt\"\n",
    "\n",
    "\n",
    "    elif data=='SemEval2015':\n",
    "        keys_input = semeval2015_keys\n",
    "        output_folder = \"WSD_Evaluation_Framework/Evaluation_Datasets/semeval2015/\"\n",
    "        orig_name = \"semeval2015.gold.key_orig.txt\"\n",
    "        clean_name = \"semeval2015.gold.key.txt\"\n",
    "\n",
    "\n",
    "    elif data=='SensEval2':\n",
    "        keys_input = senseval2_keys\n",
    "        output_folder = \"WSD_Evaluation_Framework/Evaluation_Datasets/senseval2/\"\n",
    "        orig_name = \"senseval2.gold.key_orig.txt\"\n",
    "        clean_name = \"senseval2.gold.key.txt\"\n",
    "\n",
    "\n",
    "    elif data=='SensEval3':\n",
    "        keys_input = senseval3_keys\n",
    "        output_folder = \"WSD_Evaluation_Framework/Evaluation_Datasets/senseval3/\"\n",
    "        orig_name = \"senseval3.gold.key_orig.txt\"\n",
    "        clean_name = \"senseval3.gold.key.txt\"\n",
    "\n",
    "\n",
    "    orig_path = output_folder+orig_name\n",
    "    clean_path = output_folder+clean_name\n",
    "\n",
    "    with open(orig_path,\"r\") as f_orig :\n",
    "        with open (clean_path, \"w\") as f_clean :\n",
    "            for line in keys_input :\n",
    "                line = line.strip('\\n')\n",
    "                ws_count = line.count(' ')    \n",
    "                #delete 2nd+ word sense assignment(s)\n",
    "                if ws_count > 1 :\n",
    "                    index_1 = line.find(' ')\n",
    "                    index_2 = (line[index_1+1:].find(' ')+1)+index_1\n",
    "                    line = line[:index_2]            \n",
    "                instance = line.split(\".\",2)\n",
    "                article = instance[0]\n",
    "                sentence = instance[1]\n",
    "                tag = instance[2]\n",
    "                article_num = int(article[1:])\n",
    "                f_clean.write(line+'\\n')\n",
    "    return \"Done\"\n",
    "\n",
    "\n",
    "cleanupKeyFiles('SemCor')\n",
    "cleanupKeyFiles('SemEval2007')\n",
    "cleanupKeyFiles('SemEval2013')\n",
    "cleanupKeyFiles('SemEval2015')\n",
    "cleanupKeyFiles('SensEval2')\n",
    "cleanupKeyFiles('SensEval3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD ONE-HOT ENCODING \"VOCAB\" OF WORD SENSE ASSIGNMENTS BASED ON TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"MODIFIED 8_15_17\"\"\"\n",
    "\n",
    "# Helper libraries\n",
    "from shared_lib import utils, vocabulary, ws_encoder\n",
    "\n",
    "import itertools\n",
    "reload(utils)\n",
    "reload(ws_encoder)\n",
    "\n",
    "def get_senses(keys) :\n",
    "    ## MUST USE \"CLEAN\" VERSION OF KEYS WITH JUST A SINGLE WORD SENSE\n",
    "    word_senses = [line.split(' ')[1].rstrip() for line in keys]\n",
    "    return word_senses\n",
    "    \n",
    "#get just the word sense assignments (like 'be%2:42:03::\\n')\n",
    "train_word_senses = get_senses(semcortrain_keys)\n",
    "semeval2007_test_ws = get_senses(semeval2007_keys)\n",
    "semeval2013_test_ws = get_senses(semeval2013_keys)\n",
    "semeval2015_test_ws = get_senses(semeval2015_keys)\n",
    "senseval2_test_ws = get_senses(senseval2_keys)\n",
    "senseval3_test_ws = get_senses(senseval3_keys)\n",
    "semcor_test_ws = get_senses(semcortest_keys)\n",
    "\n",
    "# --> leverages new ws_encoder package and added functions to utils.py\n",
    "#build vocab of word sense assignments\n",
    "# --> Use TRAINING data to build the vocab\n",
    "### --- WILL REFERENCE VOCAB AND INDEX THROUGHOUT DOCUMENT --- ###\n",
    "\n",
    "ws_vocab = utils.build_ws_encodings(train_word_senses)\n",
    "ws_index = ws_vocab.id_to_word_sense\n",
    "ws_reverse_index = ws_vocab.word_sense_to_id\n",
    "\n",
    "# # print counts and set of word senses :\n",
    "print \"WS ENCODINGS: WORDSET EXAMPLES\", list(itertools.islice(ws_vocab.wordset, 0, 10))\n",
    "print \n",
    "print \"WS ENCODINGS: UNIGRAM COUNTS EXAMPLES\", list(itertools.islice(ws_vocab.unigram_counts, 0, 10))\n",
    "print\n",
    "print \"WS ENCODINGS: INDEX EXAMPLES\", list(itertools.islice(ws_index.items(), 0, 10))\n",
    "print\n",
    "print \"WS ENCODINGS: REVERSE INDEX EXAMPLES\", list(itertools.islice(ws_reverse_index.items(), 0, 10))\n",
    "print\n",
    "\n",
    "#get train and test ids only of train/test words (ordered)\n",
    "train_ids = utils.preprocess_word_senses(train_word_senses, ws_vocab)\n",
    "semeval2007_test_ids = utils.preprocess_word_senses(semeval2007_test_ws, ws_vocab)\n",
    "semeval2013_test_ids = utils.preprocess_word_senses(semeval2013_test_ws, ws_vocab)\n",
    "semeval2015_test_ids = utils.preprocess_word_senses(semeval2015_test_ws, ws_vocab)\n",
    "senseval2_test_ids = utils.preprocess_word_senses(senseval2_test_ws, ws_vocab)\n",
    "senseval3_test_ids = utils.preprocess_word_senses(senseval3_test_ws, ws_vocab)\n",
    "semcor_test_ids = utils.preprocess_word_senses(semcor_test_ws, ws_vocab)\n",
    "\n",
    "#show example of train_ids\n",
    "print \"TRAIN IDS EXAMPLES:\",train_ids[0:100]\n",
    "print \n",
    "print \"TEST IDS EXAMPLES:\",semcor_test_ids[0:100]\n",
    "print\n",
    "print \"SEMEVAL2007 TEST IDS EXAMPLES:\",semeval2007_test_ids[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAKE MAP OF INSTANCE ID -> WORD SENSE/WORD SENSE INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"ADDED 8_13_17\"\"\"\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def lookup_index(word_sense, reverse_index) :\n",
    "    #looks up a word sense index number from a word sense\n",
    "    ws_index = None\n",
    "    try :\n",
    "        ws_index = reverse_index[word_sense]\n",
    "    except :\n",
    "        ws_index = 0 #reserved index number for unk\n",
    "    return ws_index\n",
    "\n",
    "def make_keys_dict(keys) :\n",
    "    #returns a dict of instance ID -> word sense\n",
    "    ws_map = dict()\n",
    "    for key in keys:\n",
    "        ID, word_sense = key.split(\" \")\n",
    "        ws_map[ID] = word_sense.rstrip()\n",
    "    return ws_map        \n",
    "\n",
    "def make_indexed_keys_dict(keys, reverse_index) :\n",
    "    #returns a dict of instance ID -> word sense index number\n",
    "    ws_map_indexed = dict()\n",
    "    for key in keys:\n",
    "        ID, word_sense = key.split(\" \")\n",
    "        ws_map_indexed[ID] = lookup_index(word_sense.rstrip(), reverse_index)\n",
    "    return ws_map_indexed\n",
    "\n",
    "#create train and test maps\n",
    "train_map = make_keys_dict(train_keys)\n",
    "test_map = make_keys_dict(test_keys)\n",
    "semeval2007_map = make_keys_dict(semeval2007_keys)\n",
    "semeval2013_map = make_keys_dict(semeval2013_keys)\n",
    "semeval2015_map = make_keys_dict(semeval2015_keys)\n",
    "senseval2_map = make_keys_dict(senseval2_keys)\n",
    "senseval3_map = make_keys_dict(senseval3_keys)\n",
    "\n",
    "train_map_indexed = make_indexed_keys_dict(train_keys, ws_reverse_index)\n",
    "test_map_indexed = make_indexed_keys_dict(test_keys, ws_reverse_index)\n",
    "\n",
    "print \"TRAIN MAP EXAMPLES:\\n\", list(itertools.islice(train_map.iteritems(), 0, 10))\n",
    "print \n",
    "print \"TRAIN MAP *INDEXED* EXAMPLES:\\n\", list(itertools.islice(train_map_indexed.iteritems(), 0, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET RELEVANT \"ROOT\" DATA FROM XML FILE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#GET MAIN XML DATA - UNCOMMENT TO GET RELEVANT DATASET\n",
    "\n",
    "#https://docs.python.org/2/library/xml.etree.elementtree.html\n",
    "#alternate good resource: https://pymotw.com/2/xml/etree/ElementTree/parse.html\n",
    "import xml.etree.ElementTree as ElementTree\n",
    "\n",
    "#SEMCOR ONLY\n",
    "semcor_root = ElementTree.parse('WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.data.xml').getroot()\n",
    "semeval2007_root = ElementTree.parse('WSD_Evaluation_Framework/Evaluation_Datasets/semeval2007/semeval2007.data.xml').getroot()\n",
    "semeval2013_root = ElementTree.parse('WSD_Evaluation_Framework/Evaluation_Datasets/semeval2013/semeval2013.data.xml').getroot()\n",
    "semeval2015_root = ElementTree.parse('WSD_Evaluation_Framework/Evaluation_Datasets/semeval2015/semeval2015.data.xml').getroot()\n",
    "senseval2_root = ElementTree.parse('WSD_Evaluation_Framework/Evaluation_Datasets/senseval2/senseval2.data.xml').getroot()\n",
    "senseval3_root = ElementTree.parse('WSD_Evaluation_Framework/Evaluation_Datasets/senseval3/senseval3.data.xml').getroot()\n",
    "#SEMCOR+OMSTI\n",
    "# semcor_omsti_root = ElementTree.parse('WSD_Evaluation_Framework/Training_Corpora/SemCor+OMSTI/semcor+omsti.gold.key.txt').getroot()\n",
    "\n",
    "###MODIFIED 8_12_17\n",
    "###TRAIN/TEST SETS :\n",
    "###COMMENT OUT IF NOT ALREADY CREATED\n",
    "semcortrain_root = ElementTree.parse('WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.data.train.xml').getroot()\n",
    "semcortest_root = ElementTree.parse('WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.data.test.xml').getroot()\n",
    "\n",
    "#SET CURRENT ROOTS\n",
    "root = semcor_root\n",
    "test_root = semcortest_root\n",
    "train_root = semcortrain_root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD TRAIN/TEST SPLIT OF SEMCOR DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \"\"\"KO MODIFIED 8/13/17 - MODIFY XML ROOT TO TRAIN/TEST\"\"\"\n",
    "\n",
    "import xml.etree.ElementTree         \n",
    "\n",
    "root_input = 'WSD_Evaluation_Framework/Training_Corpora/SemCor/semcor.data.xml'\n",
    "output_folder = \"WSD_Evaluation_Framework/Training_Corpora/SemCor/\"\n",
    "train_name = \"semcor.data.train.xml\"\n",
    "test_name = \"semcor.data.test.xml\"\n",
    "train_path = output_folder+train_name\n",
    "test_path = output_folder+test_name\n",
    "\n",
    "with open(root_input, 'r') as f:\n",
    "    text = f.readlines()\n",
    "    header = text[0:2]\n",
    "    footer = text[-1]\n",
    "\n",
    "with open(train_path, 'wb') as f:\n",
    "    with open(test_path, 'wb') as f2:\n",
    "        for line in header :\n",
    "            f.write(line)\n",
    "            f2.write(line)\n",
    "        \n",
    "        curr_text_id = None\n",
    "        curr_text_id_num = None\n",
    "        for line in text :\n",
    "            if line[0:8] == \"<text id\" :\n",
    "                curr_text_id = line.split('\"',2)[1]\n",
    "                curr_text_id_num = int(curr_text_id[1:])\n",
    "            if curr_text_id_num is not None :\n",
    "                if curr_text_id_num %100 <> 0 :\n",
    "                    f.write(line)\n",
    "                else :\n",
    "                    f2.write(line)\n",
    "        \n",
    "        for line in footer :\n",
    "            #note: goes opposite logic (because footer will already be written)\n",
    "            if not curr_text_id_num %100 <> 0 :\n",
    "                f.write(line)\n",
    "            else :\n",
    "                f2.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _LSTM MODEL:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a list for train/test that includes all words and hardcodes a sense ID for non-ambiguous terms\n",
    "\n",
    "### Non-ambiguous terms take on the sense key: 'NA_ID'+word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cscaudill/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:23: DeprecationWarning:\n",
      "\n",
      "This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Length\n",
      "180720\n",
      "180720\n",
      "\n",
      "Test Length\n",
      "9305\n",
      "9305\n",
      "\n",
      "Eval Word Lengths\n",
      "SemEval2007\t3202\n",
      "SemEval2013\t8392\n",
      "SemEval2015\t2605\n",
      "SensEval2\t5767\n",
      "SensEval3\t5542\n",
      "\n",
      "Eval Sense Lengths\n",
      "SemEval2007\t3202\n",
      "SemEval2013\t8392\n",
      "SemEval2015\t2605\n",
      "SensEval2\t5767\n",
      "SensEval3\t5542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cscaudill/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:48: DeprecationWarning:\n",
      "\n",
      "This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "\n",
      "/home/cscaudill/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:60: DeprecationWarning:\n",
      "\n",
      "This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "\n",
      "/home/cscaudill/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:71: DeprecationWarning:\n",
      "\n",
      "This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "\n",
      "/home/cscaudill/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:82: DeprecationWarning:\n",
      "\n",
      "This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "\n",
      "/home/cscaudill/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:93: DeprecationWarning:\n",
      "\n",
      "This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "\n",
      "/home/cscaudill/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:104: DeprecationWarning:\n",
      "\n",
      "This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_words_all = list()\n",
    "train_senses_all = list()\n",
    "test_words_all = list()\n",
    "test_senses_all = list()\n",
    "semeval2007_words_all = list()\n",
    "semeval2013_words_all = list()\n",
    "semeval2015_words_all = list()\n",
    "senseval2_words_all = list()\n",
    "senseval3_words_all = list()\n",
    "semeval2007_senses_all = list()\n",
    "semeval2013_senses_all = list()\n",
    "semeval2015_senses_all = list()\n",
    "senseval2_senses_all = list()\n",
    "senseval3_senses_all = list()\n",
    "\n",
    "\"\"\"KO MODIFIED: ROOT NAMES (BUT DIDN'T CHANGE WHERE FROM)\"\"\"\n",
    "\"\"\"REVIEW: MODIFY TO ALL INSTEAD OF [:2] ON FINAL TRAINING\"\"\"\n",
    "\n",
    "#Train\n",
    "train_words_all.append('<unk>')\n",
    "train_senses_all.append('<unk>')\n",
    "# for doc in semcortrain_root.getchildren()[:2]:\n",
    "for doc in semcortrain_root.getchildren()[:80]:\n",
    "    for sent in doc:\n",
    "        for text in sent:\n",
    "            train_words_all.append(text.attrib['lemma'].lower())\n",
    "            if text.tag == 'instance':\n",
    "                 train_senses_all.append(train_map[str(text.attrib['id'])].rstrip())\n",
    "            else:\n",
    "                train_senses_all.append(\"NA_ID_\")\n",
    "                \n",
    "#Test\n",
    "test_words_all.append('<unk>')\n",
    "semeval2007_words_all.append('<unk>')\n",
    "semeval2013_words_all.append('<unk>')\n",
    "semeval2015_words_all.append('<unk>')\n",
    "senseval2_words_all.append('<unk>')\n",
    "senseval3_words_all.append('<unk>')\n",
    "\n",
    "test_senses_all.append('<unk>')\n",
    "semeval2007_senses_all.append('<unk>')\n",
    "semeval2013_senses_all.append('<unk>')\n",
    "semeval2015_senses_all.append('<unk>')\n",
    "senseval2_senses_all.append('<unk>')\n",
    "senseval3_senses_all.append('<unk>')\n",
    "\n",
    "# SemCor Root\n",
    "for doc in semcortest_root.getchildren():\n",
    "    for sent in doc:\n",
    "        for text in sent:\n",
    "            test_words_all.append(text.attrib['lemma'].lower())\n",
    "            \n",
    "            if text.tag == 'instance':\n",
    "                 test_senses_all.append(test_map[str(text.attrib['id'])].rstrip())\n",
    "            else:\n",
    "                test_senses_all.append(\"NA_ID_\")                \n",
    "#                 test_senses_all.append(\"NA_ID_\"+text.attrib['lemma'].lower())\n",
    "\n",
    "# SemEval2007  \n",
    "for doc in semeval2007_root.getchildren():\n",
    "    for sent in doc:\n",
    "        for text in sent:\n",
    "            semeval2007_words_all.append(text.attrib['lemma'].lower())\n",
    "            \n",
    "            if text.tag == 'instance':\n",
    "                 semeval2007_senses_all.append(semeval2007_map[str(text.attrib['id'])].rstrip())\n",
    "            else:\n",
    "                semeval2007_senses_all.append(\"NA_ID_\")                \n",
    "            \n",
    "# SemEval2013   \n",
    "for doc in semeval2013_root.getchildren():\n",
    "    for sent in doc:\n",
    "        for text in sent:\n",
    "            semeval2013_words_all.append(text.attrib['lemma'].lower())\n",
    "            \n",
    "            if text.tag == 'instance':\n",
    "                 semeval2013_senses_all.append(semeval2013_map[str(text.attrib['id'])].rstrip())\n",
    "            else:\n",
    "                semeval2013_senses_all.append(\"NA_ID_\")                \n",
    "\n",
    "# SemEval2015               \n",
    "for doc in semeval2015_root.getchildren():\n",
    "    for sent in doc:\n",
    "        for text in sent:\n",
    "            semeval2015_words_all.append(text.attrib['lemma'].lower())\n",
    "            \n",
    "            if text.tag == 'instance':\n",
    "                 semeval2015_senses_all.append(semeval2015_map[str(text.attrib['id'])].rstrip())\n",
    "            else:\n",
    "                semeval2015_senses_all.append(\"NA_ID_\")                \n",
    "\n",
    "# SensEval2\n",
    "for doc in senseval2_root.getchildren():\n",
    "    for sent in doc:\n",
    "        for text in sent:\n",
    "            senseval2_words_all.append(text.attrib['lemma'].lower())\n",
    "            \n",
    "            if text.tag == 'instance':\n",
    "                 senseval2_senses_all.append(senseval2_map[str(text.attrib['id'])].rstrip())\n",
    "            else:\n",
    "                senseval2_senses_all.append(\"NA_ID_\")                \n",
    "            \n",
    "# SensEval3\n",
    "for doc in senseval3_root.getchildren():\n",
    "    for sent in doc:\n",
    "        for text in sent:\n",
    "            senseval3_words_all.append(text.attrib['lemma'].lower())\n",
    "            \n",
    "            if text.tag == 'instance':\n",
    "                 senseval3_senses_all.append(senseval3_map[str(text.attrib['id'])].rstrip())\n",
    "            else:\n",
    "                senseval3_senses_all.append(\"NA_ID_\")                        \n",
    "        \n",
    "print 'Train Length'\n",
    "print len(train_words_all)\n",
    "print len(train_senses_all)\n",
    "\n",
    "print '\\n'+'Test Length'\n",
    "print len(test_words_all)\n",
    "print len(test_senses_all)\n",
    "\n",
    "\n",
    "print '\\n'+'Eval Word Lengths'\n",
    "print 'SemEval2007\\t',len(semeval2007_words_all)\n",
    "print 'SemEval2013\\t',len(semeval2013_words_all)\n",
    "print 'SemEval2015\\t',len(semeval2015_words_all)\n",
    "print 'SensEval2\\t',len(senseval2_words_all)\n",
    "print 'SensEval3\\t',len(senseval3_words_all)\n",
    "\n",
    "print '\\n'+'Eval Sense Lengths'\n",
    "print 'SemEval2007\\t',len(semeval2007_senses_all)\n",
    "print 'SemEval2013\\t',len(semeval2013_senses_all)\n",
    "print 'SemEval2015\\t',len(semeval2015_senses_all)\n",
    "print 'SensEval2\\t',len(senseval2_senses_all)\n",
    "print 'SensEval3\\t',len(senseval3_senses_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_senses_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_senses_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the full train/test vocabulary to numeric IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"MODIFY TO ONLY INCLUDE TRAINING WORDS\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "lstm_train_ids = list()\n",
    "lstm_test_ids = list()\n",
    "lstm_semeval2007_ids = list()\n",
    "lstm_semeval2013_ids = list()\n",
    "lstm_semeval2015_ids = list()\n",
    "lstm_senseval2_ids = list()\n",
    "lstm_senseval3_ids = list()\n",
    "\n",
    "train_dict = {}\n",
    "\n",
    "#Create the full vocabulary\n",
    "full_words = train_words_all + test_words_all + semeval2007_words_all + semeval2013_words_all + semeval2015_words_all + senseval2_words_all + senseval3_words_all\n",
    "counter=0\n",
    "\n",
    "#generate an ID for each term and place it in a dictionary for later lookup\n",
    "# train_dict['<unk>'] = counter\n",
    "for i in train_words_all:\n",
    "    if i not in train_dict:\n",
    "        train_dict[i]=counter\n",
    "        counter+=1\n",
    "\n",
    "\n",
    "#create a list of IDs by looking up the ID from the dictionary\n",
    "for i in train_words_all:\n",
    "    lstm_train_ids.append(train_dict[i])\n",
    "for j in test_words_all:\n",
    "    try :\n",
    "        lstm_test_ids.append(train_dict[j])\n",
    "    except :\n",
    "        #Not in training dictionary, assign to <unk>\n",
    "        lstm_test_ids.append(0)\n",
    "        \n",
    "for j in semeval2007_words_all:\n",
    "    try :\n",
    "        lstm_semeval2007_ids.append(train_dict[j])\n",
    "    except :\n",
    "        #Not in training dictionary, assign to <unk>\n",
    "        lstm_semeval2007_ids.append(0)\n",
    "\n",
    "for j in semeval2013_words_all:\n",
    "    try :\n",
    "        lstm_semeval2013_ids.append(train_dict[j])\n",
    "    except :\n",
    "        #Not in training dictionary, assign to <unk>\n",
    "        lstm_semeval2013_ids.append(0)\n",
    "        \n",
    "for j in semeval2015_words_all:\n",
    "    try :\n",
    "        lstm_semeval2015_ids.append(train_dict[j])\n",
    "    except :\n",
    "        #Not in training dictionary, assign to <unk>\n",
    "        lstm_semeval2015_ids.append(0)\n",
    "        \n",
    "for j in senseval2_words_all:\n",
    "    try :\n",
    "        lstm_senseval2_ids.append(train_dict[j])\n",
    "    except :\n",
    "        #Not in training dictionary, assign to <unk>\n",
    "        lstm_senseval2_ids.append(0)\n",
    "        \n",
    "for j in senseval3_words_all:\n",
    "    try :\n",
    "        lstm_senseval3_ids.append(train_dict[j])\n",
    "    except :\n",
    "        #Not in training dictionary, assign to <unk>\n",
    "        lstm_senseval3_ids.append(0)\n",
    "        \n",
    "lstm_train_ids = np.array(lstm_train_ids)\n",
    "lstm_test_ids = np.array(lstm_test_ids)\n",
    "lstm_semeval2007_ids = np.array(lstm_semeval2007_ids)\n",
    "lstm_semeval2013_ids = np.array(lstm_semeval2013_ids)\n",
    "lstm_semeval2015_ids = np.array(lstm_semeval2015_ids)\n",
    "lstm_senseval2_ids = np.array(lstm_senseval2_ids)\n",
    "lstm_senseval3_ids = np.array(lstm_senseval3_ids)\n",
    "\n",
    "print \"TRAIN_WORDS_ALL EXAMPLES:\", train_words_all[0:5]        \n",
    "print \"TRAIN_DICT EXAMPLES:\", list(itertools.islice(train_dict.iteritems(), 0, 5))\n",
    "\n",
    "print \"TRAIN_IDS EXAMPLES (ACT LEMMAS):\", lstm_train_ids[0:30]\n",
    "print \"TEST_IDS EXAMPLES (ACT LEMMAS):\", lstm_test_ids[0:30]\n",
    "\n",
    "print 'Number of Training IDs:\\t',len(lstm_train_ids)\n",
    "print 'Number of Test IDs:\\t',len(lstm_test_ids)\n",
    "\n",
    "inv_train_dict = {v: k for k, v in train_dict.iteritems()}\n",
    "print \"INV_TRAIN_DICT EXAMPLES:\", list(itertools.islice(inv_train_dict.iteritems(), 0, 5))\n",
    "print len(train_dict.keys())\n",
    "print len(inv_train_dict.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert all senses and to numeric IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_sense_dict = {}\n",
    "lstm_train_sense_ids = list()\n",
    "lstm_test_sense_ids = list()\n",
    "lstm_semeval2007_sense_ids = list()\n",
    "lstm_semeval2013_sense_ids = list()\n",
    "lstm_semeval2015_sense_ids = list()\n",
    "lstm_senseval2_sense_ids = list()\n",
    "lstm_senseval3_sense_ids = list()\n",
    "\n",
    "# Create the full vocabulary\n",
    "# lstm_senses = train_senses_all + test_senses_all\n",
    "counter=0\n",
    "\n",
    "#generate an ID for each term and place it in a dictionary for later lookup\n",
    "# train_sense_dict['<unk>']=counter\n",
    "for ws in train_senses_all:\n",
    "    if ws not in train_sense_dict:\n",
    "        train_sense_dict[ws]=counter\n",
    "        counter+=1      \n",
    "\n",
    "#create a list of IDs by looking up the ID from the dictionary\n",
    "for train_ws in train_senses_all:\n",
    "    lstm_train_sense_ids.append(train_sense_dict[train_ws])\n",
    "for test_ws in test_senses_all:\n",
    "    try :\n",
    "        lstm_test_sense_ids.append(train_sense_dict[test_ws])\n",
    "    except :\n",
    "        #assign to <unk>\n",
    "        lstm_test_sense_ids.append(0)\n",
    "\n",
    "# SemEval2007\n",
    "for s2007_ws in semeval2007_senses_all:\n",
    "    try :\n",
    "        lstm_semeval2007_sense_ids.append(train_sense_dict[s2007_ws])\n",
    "    except :\n",
    "        lstm_semeval2007_sense_ids.append(0)\n",
    "\n",
    "# SemEval2013\n",
    "for s2013_ws in semeval2013_senses_all:\n",
    "    try :\n",
    "        lstm_semeval2013_sense_ids.append(train_sense_dict[s2013_ws])\n",
    "    except :\n",
    "        lstm_semeval2013_sense_ids.append(0)\n",
    "        \n",
    "# SemEval2015\n",
    "for s2015_ws in semeval2015_senses_all:\n",
    "    try :\n",
    "        lstm_semeval2015_sense_ids.append(train_sense_dict[s2015_ws])\n",
    "    except :\n",
    "        #assign to <unk>\n",
    "        lstm_semeval2015_sense_ids.append(0)\n",
    "        \n",
    "# SensEval2\n",
    "for s2_ws in senseval2_senses_all:\n",
    "    try :\n",
    "        lstm_senseval2_sense_ids.append(train_sense_dict[s2_ws])\n",
    "    except :\n",
    "        #assign to <unk>\n",
    "        lstm_senseval2_sense_ids.append(0)\n",
    "        \n",
    "# SensEval3\n",
    "for s3_ws in senseval3_senses_all:\n",
    "    try :\n",
    "        lstm_senseval3_sense_ids.append(train_sense_dict[s3_ws])\n",
    "    except :\n",
    "        #assign to <unk>\n",
    "        lstm_senseval3_sense_ids.append(0)\n",
    "\n",
    "    \n",
    "lstm_train_sense_ids = np.array(lstm_train_sense_ids)\n",
    "lstm_test_sense_ids = np.array(lstm_test_sense_ids)\n",
    "lstm_semeval2007_sense_ids = np.array(lstm_semeval2007_sense_ids)\n",
    "lstm_semeval2013_sense_ids = np.array(lstm_semeval2013_sense_ids)\n",
    "lstm_semeval2015_sense_ids = np.array(lstm_semeval2015_sense_ids)\n",
    "lstm_senseval2_sense_ids = np.array(lstm_senseval2_sense_ids)\n",
    "lstm_senseval3_sense_ids = np.array(lstm_senseval3_sense_ids)\n",
    "\n",
    "print \"train_senses_all EXAMPLES:\", train_senses_all[0:3]        \n",
    "print \"train_sense_dict EXAMPLES:\", list(itertools.islice(train_sense_dict.iteritems(), 0, 3))\n",
    "\n",
    "print len(test_senses_all)\n",
    "print \"lstm_train_sense_ids EXAMPLES (ACT SENSES):\", lstm_train_sense_ids[0:20]\n",
    "print \"lstm_test_sense_ids (ACT SENSES - PER TRAINING):\", lstm_test_sense_ids[0:20]\n",
    "print \"length lstm_test_sense_ids:\", len(lstm_test_sense_ids)\n",
    "\n",
    "print 'Number of Training Sense IDs:\\t',len(lstm_train_sense_ids)\n",
    "print 'Number of Test Sense IDs:\\t',len(lstm_test_sense_ids)\n",
    "\n",
    "inv_train_sense_dict = {v: k for k, v in train_sense_dict.iteritems()}\n",
    "print \"inv_train_sense_dict EXAMPLES:\", list(itertools.islice(inv_train_sense_dict.iteritems(), 0, 5))\n",
    "\n",
    "print inv_train_sense_dict[14]\n",
    "print len(train_sense_dict.keys())\n",
    "print len(inv_train_sense_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#create dictionary with order_index encountered, (lemma, ws)\n",
    "train_order_dict = {}\n",
    "for order_index, (lemma, ws) in enumerate(zip(train_words_all,train_senses_all)) :\n",
    "    train_order_dict[order_index] = lemma, ws\n",
    "\n",
    "print len(train_words_all)\n",
    "print len(train_senses_all)\n",
    "    \n",
    "print \"TRAINING ORDER DICT: \", list(itertools.islice(train_order_dict.iteritems(), 0, 5))\n",
    "\n",
    "#create train_word_senses_counts dictionary (used after for getting set of word senses per lemma)\n",
    "train_word_senses_counts_dict = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for order_index, (lemma, ws) in train_order_dict.iteritems():\n",
    "    train_word_senses_counts_dict[lemma][ws]+=1\n",
    "print \n",
    "print \"TRAINING WORD SENSES COUNTS DICT: \", list(itertools.islice(train_word_senses_counts_dict.iteritems(), 0, 5))\n",
    "    \n",
    "#get set of word senses per lemma encountered\n",
    "train_word_senses_dict = defaultdict(None)\n",
    "for lemma, ws_counts in train_word_senses_counts_dict.iteritems() :\n",
    "    train_word_senses_dict[lemma] = ws_counts.keys()\n",
    "print\n",
    "print \"TRAINING WORD SENSES DICT: \", list(itertools.islice(train_word_senses_dict.iteritems(), 0, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print lstm_train_ids[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print lstm_train_sense_ids[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to take the numeric IDs from above and lookup the corresponding word/sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"REDO AND USE ORDER_DICT AND KEYS\"\n",
    "\n",
    "def RNN_WordLookup(train_orderID):\n",
    "    try :\n",
    "        return train_order_dict[train_orderID][0]\n",
    "    except :\n",
    "        return None\n",
    "          \n",
    "def RNN_SenseLookup(train_orderID):   \n",
    "    try :\n",
    "        return train_order_dict[train_orderID][1]\n",
    "    except :\n",
    "        return None\n",
    "\n",
    "      \n",
    "\"RNN WORD, SENSE LOOKUPS FOR A SPECIFIC ID #\"\n",
    "print RNN_WordLookup(45)\n",
    "print RNN_SenseLookup(45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json, os, re, shutil, sys, time\n",
    "import collections, itertools\n",
    "import unittest\n",
    "from IPython.display import display, HTML\n",
    "import operator\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.1\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries\n",
    "from shared_lib import utils, vocabulary, tf_embed_viz, ws_encoder\n",
    "\n",
    "# Your code\n",
    "import rnnlm\n",
    "reload(rnnlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show how the batches feed into LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "html = \"<h3>Input words w:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "\n",
    "bi = utils.custom_batch_generator(lstm_train_ids[:40], lstm_train_sense_ids[:40], batch_size=4, max_time=5)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    html += \"<td>\" + utils.render_matrix(w, cols=[\"w_%d\" % d for d in range(w.shape[1])], dtype=object) + \"</td>\"\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))\n",
    "\n",
    "html = \"<h3>Target senses y:</h3>\"\n",
    "html += \"<table><tr><th>Batch 0</th><th>Batch 1</th></tr><tr>\"\n",
    "bi = utils.custom_batch_generator(lstm_train_ids[:40], lstm_train_sense_ids[:40], batch_size=4, max_time=5)\n",
    "for i, (w,y) in enumerate(bi):\n",
    "    html += \"<td>\" + utils.render_matrix(y, cols=[\"y_%d\" % d for d in range(y.shape[1])], dtype=object) + \"</td>\"\n",
    "html += \"</tr></table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RNN Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_epoch(lm, session, batch_iterator,\n",
    "              train=False, verbose=True,\n",
    "              tick_s=10, learning_rate=0.1):\n",
    "    start_time = time.time()\n",
    "    tick_time = start_time  # for showing status\n",
    "    total_cost = 0.0  # total cost, summed over all words\n",
    "    total_batches = 0\n",
    "    total_words = 0\n",
    "    logs = list()\n",
    "    sm_logs = list()\n",
    "    ins = list()\n",
    "    proba_list = list()\n",
    "    pmax_list = list()\n",
    "    prand_list = list()\n",
    "\n",
    "    if train:\n",
    "        train_op = lm.train_step_\n",
    "        use_dropout = True\n",
    "        loss = lm.train_loss_\n",
    "    else:\n",
    "        train_op = tf.no_op()\n",
    "        use_dropout = False  # no dropout at test time\n",
    "        loss = lm.loss_  # true loss, if train_loss is an approximation\n",
    "\n",
    "    for i, (w, y) in enumerate(batch_iterator):\n",
    "        cost = 0.0\n",
    "        # At first batch in epoch, get a clean intitial state.\n",
    "        if i == 0:\n",
    "            h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
    "\n",
    "        #### YOUR CODE HERE ####\n",
    "        feed_dict = {lm.input_w_:w,\n",
    "                     lm.target_y_:y,\n",
    "                     lm.initial_h_:h,\n",
    "                     lm.learning_rate_:learning_rate,\n",
    "                     lm.use_dropout_:use_dropout}\n",
    "        \n",
    "        ### KO MODIFIED\n",
    "        if not train :\n",
    "            h, cost,sm_logits,_ = session.run([lm.final_h_, loss, lm.softmax_logits_, train_op], feed_dict=feed_dict)          \n",
    "        else :\n",
    "            h, cost,_ = session.run([lm.final_h_, loss, train_op], feed_dict=feed_dict)\n",
    "        \n",
    "        total_cost += cost\n",
    "        total_batches = i + 1\n",
    "        total_words += w.size  # w.size = batch_size * max_time\n",
    "        \n",
    "        #KO MODIFIED \n",
    "        if not train :\n",
    "#             logs.append(logits)\n",
    "            sm_logs.append(sm_logits)\n",
    "        \n",
    "        ##\n",
    "        # Print average loss-so-far for epoch\n",
    "        # If using train_loss_, this may be an underestimate.\n",
    "        if verbose and (time.time() - tick_time >= tick_s):\n",
    "            avg_cost = total_cost / total_batches\n",
    "            avg_wps = total_words / (time.time() - start_time)\n",
    "            print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (\n",
    "                i, total_words, avg_wps, avg_cost)\n",
    "            tick_time = time.time()  # reset time ticker\n",
    "            \n",
    "    #KO MODIFIED\n",
    "    if not train :\n",
    "        return (total_cost / total_batches), sm_logs\n",
    "    else :\n",
    "        return (total_cost / total_batches)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(lm, session, word_ids,sense_ids, name=\"Data\"):\n",
    "    # For scoring, we can use larger batches to speed things up.\n",
    "    bi = utils.custom_batch_generator(word_ids, sense_ids, batch_size=100, max_time=100)\n",
    "    cost = run_epoch(lm, session, bi, \n",
    "                     learning_rate=1.0, train=False, \n",
    "                     verbose=False, tick_s=3600)[0]\n",
    "    print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define LSTM Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_time = 100\n",
    "batch_size = 30\n",
    "learning_rate = 0.2\n",
    "### MODIFIED to go faster\n",
    "num_epochs = 10\n",
    "# num_epochs = 30\n",
    "\n",
    "# Model parameters\n",
    "model_params = dict(V=len(train_senses_all),\n",
    "                    H=400, \n",
    "                    softmax_ns=200,\n",
    "                    num_layers=4)\n",
    "\n",
    "TF_SAVEDIR = \"tf_saved\"\n",
    "checkpoint_filename = os.path.join(TF_SAVEDIR, \"rnnlm\")\n",
    "trained_filename = os.path.join(TF_SAVEDIR, \"rnnlm_trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reload(rnnlm)\n",
    "# Will print status every this many seconds\n",
    "print_interval = 5\n",
    "\n",
    "# Clear old log directory\n",
    "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
    "\n",
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "# Clear old log directory\n",
    "shutil.rmtree(TF_SAVEDIR, ignore_errors=True)\n",
    "if not os.path.isdir(TF_SAVEDIR):\n",
    "    os.makedirs(TF_SAVEDIR)\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    session.run(initializer)\n",
    "\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        bi = utils.custom_batch_generator(lstm_train_ids, lstm_train_sense_ids, batch_size, max_time)\n",
    "        print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
    "\n",
    "        # Run a training epoch.\n",
    "        ### KO MODIFIED: REMOVE LOGITS\n",
    "        cost = run_epoch(lm,session,bi,train=True, learning_rate=learning_rate)\n",
    "        \n",
    "        print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "        # Save a checkpoint\n",
    "        saver.save(session, checkpoint_filename, global_step=epoch)\n",
    "        \n",
    "#         print (\"[epoch %d]\" % epoch),\n",
    "#         score_dataset(lm, session, lstm_train_ids,lstm_train_sense_ids, name=\"Train set\")\n",
    "#         print (\"[epoch %d]\" % epoch),\n",
    "#         score_dataset(lm, session, lstm_test_ids, lstm_test_sense_ids, name=\"Test set\")\n",
    "#         print \"\"\n",
    "    \n",
    "    # Save final model\n",
    "\n",
    "    saver.save(session, trained_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on Test data\n",
    "\n",
    "## In the cell below, remove the comments for the evaluation set that you'd like to test with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(senseval3_test_ids)\n",
    "print len(lstm_senseval3_ids)\n",
    "print len(lstm_senseval3_sense_ids)\n",
    "print len(senseval3_words_all)\n",
    "print len(senseval3_senses_all)\n",
    "print len(senseval3_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2532\n"
     ]
    }
   ],
   "source": [
    "def selectTestData(data):\n",
    "\n",
    "    if data == 'SemCorTest':\n",
    "        return semcor_test_ids,lstm_test_ids,lstm_test_sense_ids,test_words_all,test_senses_all,semcortest_root\n",
    "\n",
    "    elif data == 'SemEval2007':\n",
    "        return semeval2007_test_ids,lstm_semeval2007_ids,lstm_semeval2007_sense_ids,semeval2007_words_all,semeval2007_senses_all,semeval2007_root\n",
    "\n",
    "    elif data == 'SemEval2013':\n",
    "        return semeval2013_test_ids,lstm_semeval2013_ids,lstm_semeval2013_sense_ids,semeval2013_words_all,semeval2013_senses_all,semeval2013_root\n",
    "\n",
    "    elif data == 'SemEval2015': \n",
    "        return semeval2015_test_ids,lstm_semeval2015_ids,lstm_semeval2015_sense_ids,semeval2015_words_all,semeval2015_senses_all,semeval2015_root\n",
    "\n",
    "    elif data == 'SensEval2':\n",
    "        return senseval2_test_ids,lstm_senseval2_ids,lstm_senseval2_sense_ids,senseval2_words_all,senseval2_senses_all,senseval2_root\n",
    "\n",
    "    elif data == 'SensEval3':\n",
    "        return senseval3_test_ids,lstm_senseval3_ids,lstm_senseval3_sense_ids,senseval3_words_all,senseval3_senses_all,senseval3_root\n",
    "    \n",
    "\n",
    "var_test_ids,var_lstm_test_ids,var_test_sense_ids,var_test_words_all,var_test_senses_all,var_test_root = selectTestData('SemCorTest')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"CREATE ORDER DICT FOR EVERY TEST/EVAL SET TO CREATE PREDICTIVE OUTPUT IN LSTM\"\"\"\n",
    "test_order_dict = {}\n",
    "for order_index, (lemma, ws) in enumerate(zip(var_test_words_all,var_test_senses_all)) :\n",
    "    test_order_dict[order_index] = lemma, ws\n",
    "\n",
    "print len(var_test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tf_saved/rnnlm_trained\n",
      "Starting epoch\n",
      "Completed in 0:00:08\n",
      "Test set: avg. loss: 3.136  (perplexity: 23.01)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm = rnnlm.RNNLM(**model_params)\n",
    "lm.BuildCoreGraph()\n",
    "# # lm.BuildTrainGraph()\n",
    "lm.BuildSamplerGraph()\n",
    "\n",
    "# Explicitly add global initializer and variable saver to LM graph\n",
    "with lm.graph.as_default():\n",
    "    initializer = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=lm.graph) as session:\n",
    "    # Seed RNG for repeatability\n",
    "    tf.set_random_seed(42)\n",
    "\n",
    "    saver.restore(session, trained_filename)\n",
    "\n",
    "    t0_epoch = time.time()\n",
    "    bi = utils.custom_batch_generator(var_lstm_test_ids, var_test_sense_ids, batch_size, max_time)\n",
    "    print \"Starting epoch\"\n",
    "    # Run a training epoch.\n",
    "    \"\"\"KO MODIFIED TO _TEST\"\"\"\n",
    "    cost, sm_logits_test = run_epoch(lm,session,bi,train=False, learning_rate=learning_rate)\n",
    "    print \"Completed in %s\" % (utils.pretty_timedelta(since=t0_epoch))\n",
    "    \n",
    "#     print (\"[epoch %d]\" % epoch),\n",
    "#     score_dataset(lm, session, lstm_train_ids, lstm_train_sense_ids, name=\"Train set\")\n",
    "    score_dataset(lm, session, var_lstm_test_ids, var_test_sense_ids, name=\"Test set\")\n",
    "    print \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSSIBLE SENSES:\n",
      "['NA_ID_']\n",
      "['make%2:36:08::', 'make%2:29:08::', 'make%2:36:11::', 'make%2:41:00::', 'make%2:36:13::', 'make%2:30:00::', 'make%2:31:13::', 'make%2:36:15::', 'make%2:41:03::', 'make%2:30:02::', 'make%2:38:00::', 'make%2:32:00::', 'make%2:36:01::', 'make%2:42:05::', 'make%2:36:00::', 'make%2:42:00::', 'make%2:31:00::', 'make%2:40:02::', 'make%2:40:01::', 'make%1:09:00::', 'make%2:38:02::', 'make%2:36:09::', 'NA_ID_']\n",
      "None\n",
      "WORD SENSE IDs:\n",
      "set([82, 339, 324, 69, 29])\n",
      "set([])\n"
     ]
    }
   ],
   "source": [
    "def returnPossibleSenses(word) :\n",
    "    #from a word like \"make\" look up all possible word senses\n",
    "    try :\n",
    "        return train_word_senses_dict[word]\n",
    "    except :\n",
    "        return None\n",
    "\n",
    "def returnWordSenseIDs(word_senses) :\n",
    "    #returns all WORD SENSE IDs for list of word senses ['make%2:36:00::\\n', 'make%2:31:13::\\n']\n",
    "    word_sense_IDs = list()\n",
    "    for word_sense in word_senses :\n",
    "        \n",
    "        if word_sense in train_sense_dict :\n",
    "            word_sense_IDs.append(train_sense_dict[word_sense])\n",
    "        else :\n",
    "            continue\n",
    "    word_sense_IDs = set(word_sense_IDs)\n",
    "    return word_sense_IDs\n",
    "\n",
    "print \"POSSIBLE SENSES:\"\n",
    "print returnPossibleSenses('how')\n",
    "print returnPossibleSenses('make')\n",
    "print returnPossibleSenses('qwerty')\n",
    "\n",
    "print \"WORD SENSE IDs:\"\n",
    "print returnWordSenseIDs(['make%2:36:00::', 'make%2:31:13::', 'make%2:41:00::', 'make%2:42:00::', 'make%2:30:00::', 'NA_ID_make', 'NA_ID_made'])\n",
    "print returnWordSenseIDs(['qwerty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a dictionary of logits for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "INDEX_ID 5520\n",
      "LOGITS TEST DICT EXAMPLES: {'man%1:18:04::': 0.011846504099500939, 'man%1:05:01::': 2.3607048656445064e-07, 'man%1:18:08::': 2.3239614571941497e-07, 'man%1:18:03::': 0.0023169844453728695, 'man%1:18:00::': 0.98583604298849403}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "order_index=0\n",
    "\n",
    "## Logit data is output in many embedded arrays\n",
    "## Number of terms in each back is equal to -> (max_time * batch_length)\n",
    "\n",
    "print order_index\n",
    "\n",
    "\n",
    "#MODIFY ORDERED_DICT AS APPROPRIATE\n",
    "def return_sense_id_preds(logits, possible_senses) :\n",
    "\n",
    "    preds_dict = None\n",
    "    if possible_senses is None :\n",
    "        return preds_dict\n",
    "        \n",
    "    word_sense_indices = []\n",
    "    for word_sense in possible_senses :\n",
    "        \n",
    "        #exclude NA_IDs (non-instances) from normalization\n",
    "        if word_sense[0:5] == \"NA_ID\" :\n",
    "            continue\n",
    "        else :\n",
    "            #get word sense index, append to possible indices\n",
    "            try :\n",
    "                word_sense_index = train_sense_dict[word_sense]\n",
    "                word_sense_indices.append(word_sense_index)\n",
    "            except :\n",
    "                continue\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    \n",
    "    for word_sense_index in word_sense_indices :\n",
    "        try :\n",
    "            total += logits[word_sense_index]\n",
    "        except :\n",
    "            continue\n",
    "    #normalize\n",
    "    try :\n",
    "        normalizer = 1/float(total)\n",
    "    except :\n",
    "        normalizer = 1\n",
    "    \n",
    "    #EDITED 8_19_17_KO \n",
    "    if word_sense_indices <> [] :\n",
    "        preds_dict = {}\n",
    "\n",
    "        #normalize\n",
    "        for word_sense_index in word_sense_indices :\n",
    "            preds_dict[inv_train_sense_dict[word_sense_index]] = logits[word_sense_index]*normalizer\n",
    "    \n",
    "#     print preds_dict\n",
    "    return preds_dict\n",
    "\n",
    "# print return_sense_id_preds(sm_logits_test[0][0][1], \n",
    "#                             returnPossibleSenses(test_order_dict[7][0]))\n",
    "        \n",
    "\n",
    "\"\"\"COPY/PASTE FOR EVAL SETS AS APPROPRIATE\n",
    "sm_logits_dict_test\n",
    "sm_logits_test (shows twice)\n",
    "test_order_dict\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sm_logits_dict_test = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "\n",
    "batch_num = 0\n",
    "index_id = 0\n",
    "for batch in sm_logits_test:\n",
    "    word_num = 0\n",
    "    for word in batch:\n",
    "        log_num = 0\n",
    "        for logs in word:\n",
    "            sm_logits_dict_test[index_id] = return_sense_id_preds(logits=sm_logits_test[batch_num][word_num][log_num],\n",
    "                                                 possible_senses=returnPossibleSenses(test_order_dict[index_id][0]))\n",
    "            \n",
    "#             if index_id == 1 :\n",
    "#                 print \"LOGITS:\", sm_logits_test[batch_num][word_num][log_num]\n",
    "#                 print \"POSSIBLE SENSES:\", returnPossibleSenses(test_order_dict[index_id][1])\n",
    "#                 print \"RETURN PREDS DICT: \", return_sense_id_preds(sm_logits_test[batch_num][word_num][log_num],\n",
    "#                                                                   returnPossibleSenses(test_order_dict[index_id][0]))\n",
    "\n",
    "            index_id += 1\n",
    "            log_num += 1\n",
    "        word_num += 1\n",
    "    batch_num += 1\n",
    "\n",
    "print \"INDEX_ID\", index_id\n",
    "print \"LOGITS TEST DICT EXAMPLES:\", sm_logits_dict_test[5]\n",
    "\n",
    "# print \"LOGITS TEST DICT EXAMPLES:\", list(itertools.islice(sm_logits_dict_test.iteritems(), 2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Accuracy when predicting within only the available senses from a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5520\n",
      "5542\n",
      "[0 1 1 ..., 1 1 1]\n",
      "5520\n",
      "613\n",
      "% CORRECT (INTENTIONALLY MARKS ALL NON-INSTANCE IDs AS WRONG, SO LOW SCORE: 0.111050724638\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "counter = 0\n",
    "correct = 0\n",
    "\n",
    "correct_index_list = []\n",
    "correct_word_sense_list = []\n",
    "\n",
    "print len(sm_logits_dict_test.keys())\n",
    "print len(var_test_sense_ids)\n",
    "print var_test_sense_ids\n",
    "\n",
    "#note the OFFSET[:1] in lstm_test_sense_ids, as index 0 is a manually-inserted '<unk>'.\n",
    "#May have to continue this in other combination sections as well\n",
    "for (k, preds), act_word_sense_index in zip(sm_logits_dict_test.iteritems(), var_test_sense_ids) :\n",
    "    counter += 1\n",
    "    #get maximum probability, maximum word_sense (sorted makes default to first index if there is a tie)\n",
    "    try :\n",
    "        max_word_sense = max(sorted(preds.iteritems()), key=operator.itemgetter(1))[0]\n",
    "        max_word_sense_index = train_sense_dict[max_word_sense]\n",
    "        if max_word_sense_index == act_word_sense_index :\n",
    "            correct += 1\n",
    "            correct_index_list.append(max_word_sense_index)\n",
    "            correct_word_sense_list.append(max_word_sense)\n",
    "    \n",
    "        else :\n",
    "            pass\n",
    "    except : #handle empty sets\n",
    "        continue\n",
    "        \n",
    "print counter\n",
    "print correct\n",
    "\n",
    "print \"% CORRECT (INTENTIONALLY MARKS ALL NON-INSTANCE IDs AS WRONG, SO LOW SCORE:\", float(correct)/counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in correct_word_sense_list :\n",
    "    if i[0:5] <> 'NA_ID' :\n",
    "        counter += 1\n",
    "        \n",
    "print counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GET INSTANCE_IDS for TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM TEST OUTPUT (PREDS_DENSE): [('d002.s124.t001', {'house%1:04:00::': 0.59613375942387647, 'house%2:42:02::': 0.12096509549458763, 'house%2:41:00::': 0.1776297143996825, 'house%1:06:00::': 0.10527143068185335}), ('d002.s034.t000', {'huge%5:00:01:large:00': 1.0}), ('d002.s034.t001', {'fire%2:33:01::': 0.045921063652148597, 'fire%1:22:00::': 0.0045605937990364515, 'fire%1:11:00::': 0.12780875284691645, 'fire%1:04:00::': 0.022272275143687039, 'fire%2:41:00::': 0.060677808526567699, 'fire%2:33:00::': 0.73875950603164364}), ('d002.s034.t002', None), ('d001.s027.t007', {'work%1:06:01::': 0.094509294376201217, 'work%2:41:00::': 0.037240508042546513, 'work%2:41:03::': 0.019445928564722718, 'work%2:35:02::': 0.00098776325423003525, 'work%1:04:00::': 0.16742356173123224, 'work%1:06:00::': 0.0013224919830759324, 'work%2:41:04::': 0.011835364019038633, 'work%2:41:02::': 0.024488503679619439, 'work%1:04:01::': 0.012996645297749096, 'work%1:09:00::': 0.60609318447394034, 'work%2:36:00::': 0.0081557106197149599, 'work%2:41:05::': 0.01550104395792876})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cscaudill/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:7: DeprecationWarning:\n",
      "\n",
      "This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counter = 1  #starts at 1 to skip \"<unk>\" at index 0\n",
    "lstm_dense_preds_test = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "### NOTE: THIS IS ON [0:2] TO MATCH UP TO WHAT TEST OUTPUT WAS; MODIFY TO FULL SET AS APPROPRIATE\n",
    "\n",
    "instance_counter = 0\n",
    "for doc in var_test_root.getchildren()[:]:\n",
    "    for sent in doc:\n",
    "        for text in sent:\n",
    "            if text.tag == 'instance':\n",
    "                instance_counter += 1\n",
    "                lstm_dense_preds_test[str(text.attrib['id'])] = sm_logits_dict_test[counter]\n",
    "#                 print str(text.attrib['id']),str(text.attrib['lemma']), counter, sm_logits_dict_test[counter]\n",
    "            else :\n",
    "                pass            \n",
    "            counter += 1\n",
    "\n",
    "# print instance_counter\n",
    "print \"LSTM TEST OUTPUT (PREDS_DENSE):\", list(itertools.islice(lstm_dense_preds_test.iteritems(), 0, 5))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open (\"lstm_output.txt\", \"w\") as f :\n",
    "    for line in sorted(lstm_dense_preds_test.iteritems()) :\n",
    "        f.write(str(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Conversion to Ensemble Vocab Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d002.s124.t001', {'house%1:04:00::': 0.59613375942387647, 'house%2:42:02::': 0.12096509549458763, 'house%2:41:00::': 0.1776297143996825, 'house%1:06:00::': 0.10527143068185335}), ('d002.s034.t000', {'huge%5:00:01:large:00': 1.0}), ('d002.s034.t001', {'fire%2:33:01::': 0.045921063652148597, 'fire%1:22:00::': 0.0045605937990364515, 'fire%1:11:00::': 0.12780875284691645, 'fire%1:04:00::': 0.022272275143687039, 'fire%2:41:00::': 0.060677808526567699, 'fire%2:33:00::': 0.73875950603164364}), ('d002.s034.t002', None), ('d001.s027.t007', {'work%1:06:01::': 0.094509294376201217, 'work%2:41:00::': 0.037240508042546513, 'work%2:41:03::': 0.019445928564722718, 'work%2:35:02::': 0.00098776325423003525, 'work%1:04:00::': 0.16742356173123224, 'work%1:06:00::': 0.0013224919830759324, 'work%2:41:04::': 0.011835364019038633, 'work%2:41:02::': 0.024488503679619439, 'work%1:04:01::': 0.012996645297749096, 'work%1:09:00::': 0.60609318447394034, 'work%2:36:00::': 0.0081557106197149599, 'work%2:41:05::': 0.01550104395792876})]\n",
      "[('d002.s034.t000', {2191: 1.0}), ('d002.s034.t001', {5825: 0.060677808526567699, 5212: 0.0045605937990364515, 1647: 0.045921063652148597, 3858: 0.73875950603164364, 3928: 0.022272275143687039, 2076: 0.12780875284691645}), ('d002.s034.t002', None), ('d001.s027.t007', {4416: 0.0081557106197149599, 354: 0.024488503679619439, 10118: 0.094509294376201217, 9322: 0.011835364019038633, 270: 0.16742356173123224, 1264: 0.012996645297749096, 1074: 0.019445928564722718, 1811: 0.00098776325423003525, 308: 0.0013224919830759324, 12430: 0.01550104395792876, 3447: 0.60609318447394034, 447: 0.037240508042546513}), ('d001.s027.t006', {870: 0.8332594005519155, 5806: 0.12930418797294194, 2878: 0.037436411475142518}), ('d001.s027.t005', None), ('d001.s027.t004', {5088: 0.15351812553213834, 878: 0.058473086537859202, 598: 0.78800878793000251}), ('d001.s027.t003', {1709: 0.99999999999999989}), ('d001.s027.t002', {7920: 0.76825641816366441, 1844: 0.23174358183633553}), ('d001.s027.t001', {5721: 0.15157659358767192, 6138: 0.24052658814377595, 3894: 0.60789681826855213})]\n",
      "1850\n"
     ]
    }
   ],
   "source": [
    "###ADDED 8_19_17_KO\n",
    "\n",
    "def lstm_sense_dict_to_ensemble_index(lstm_dict, reverse_index) :\n",
    "    print  list(itertools.islice(lstm_dict.iteritems(), 0, 5)) \n",
    "    lstm_dict_index = defaultdict(dict)\n",
    "    for ID, preds in lstm_dict.iteritems() :\n",
    "        if preds is None or len(preds.keys()) == 0 :\n",
    "            lstm_dict_index[ID] = None\n",
    "        else :\n",
    "            for ws, prob in preds.iteritems() :\n",
    "                ws_index = lookup_index(ws, reverse_index)\n",
    "                lstm_dict_index[ID][ws_index] = prob\n",
    "    return lstm_dict_index\n",
    "\n",
    "lstm_dense_preds_test_matched_index = lstm_sense_dict_to_ensemble_index(lstm_dict=lstm_dense_preds_test,\n",
    "                                                                       reverse_index=ws_reverse_index)\n",
    "  \n",
    "print list(itertools.islice(lstm_dense_preds_test_matched_index.iteritems(), 0, 10))  \n",
    "\n",
    "# for k in lstm_dense_preds_test.iterkeys() :\n",
    "#     if k not in lstm_dense_preds_test_matched_index.keys() :\n",
    "#         print k, lstm_dense_preds_test[k]\n",
    "\n",
    "print len(lstm_dense_preds_test_matched_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# _MAKE BIGRAM/TRIGRAM MODELS:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE CONTEXT COUNTS\n",
    "##### NOTE:\n",
    " - INCLUDING CURRENT LEMMA IN CONTEXT COUNT, AS WE WANT TO LIMIT TO *ONLY* WORD SENSES FOR THAT LEMMA\n",
    " - CONTEXT INFORMATION ACTUALLY INCLUDES CURRENT LEMMA/POS INFO SINCE WE ARE GIVEN THIS IN TEXT AND WANT TO PREDICT WORD SENSE (*NOT* NEXT-WORD PREDICTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cscaudill/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:23: DeprecationWarning:\n",
      "\n",
      "This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "\n",
      "/home/cscaudill/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:30: DeprecationWarning:\n",
      "\n",
      "This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "\n",
      "/home/cscaudill/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:32: DeprecationWarning:\n",
      "\n",
      "This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS (lemmas): [('fawn', defaultdict(<function <lambda> at 0x7f5b9da8a488>, {27399: 1.0})), ('xylem', defaultdict(<function <lambda> at 0x7f5bf98e07d0>, {9287: 4.0})), ('unscientific', defaultdict(<function <lambda> at 0x7f4f35f0d668>, {15671: 2.0})), ('writings', defaultdict(<function <lambda> at 0x7f4f34c17668>, {26850: 1.0})), ('systematic', defaultdict(<function <lambda> at 0x7f4f39df2cf8>, {16728: 2.0}))]\n",
      "\n",
      "BIGRAMS (lemmas): [(('helium', 'temperature'), defaultdict(<function <lambda> at 0x7f4f4b7c8e60>, {221: 1.0})), (('little', 'note'), defaultdict(<function <lambda> at 0x7f5bf81c7b18>, {3058: 1.0})), (('recently', 'return'), defaultdict(<function <lambda> at 0x7f5bd6c7fc08>, {187: 1.0})), (('man', 'receive'), defaultdict(<function <lambda> at 0x7f5b4dcf7ed8>, {142: 1.0})), (('already', 'pupate'), defaultdict(<function <lambda> at 0x7f4f3ae00938>, {14571: 1.0}))]\n",
      "\n",
      "TRIGRAMS (lemmas): [(('at', 'the', 'beginning'), defaultdict(<function <lambda> at 0x7f5b7bf0fc08>, {6299: 3.0, 2269: 4.0, 4662: 1.0})), (('the', 'gang', 'be'), defaultdict(<function <lambda> at 0x7f4f4947ded8>, {1: 1.0})), (('devout', ',', 'orthodox'), defaultdict(<function <lambda> at 0x7f5b7a1c1c08>, {10035: 1.0})), (('for', 'some', 'form'), defaultdict(<function <lambda> at 0x7f620d6fd8c0>, {420: 1.0})), (('might', 'be', 'crippling'), defaultdict(<function <lambda> at 0x7f5b792d42a8>, {20269: 1.0}))]\n",
      "\n",
      "UNIGRAMS (POS|curr lemma): [(('VERB', 'buck_up'), defaultdict(<function <lambda> at 0x7f5bd4545410>, {33118: 1.0})), (('NOUN', 'starting_point'), defaultdict(<function <lambda> at 0x7f5bf4891b18>, {13345: 2.0})), (('VERB', 'breathe'), defaultdict(<function <lambda> at 0x7f5bf7603a28>, {2217: 17.0, 28291: 1.0, 29133: 1.0})), (('VERB', 'usher'), defaultdict(<function <lambda> at 0x7f5bd67532a8>, {14282: 2.0})), (('ADJ', 'greater'), defaultdict(<function <lambda> at 0x7f5b4f1c4c80>, {747: 41.0}))]\n",
      "\n",
      "BIGRAMS (POS|curr lemma): [(('DET', 'NOUN', 'shock_therapy'), defaultdict(<function <lambda> at 0x7f4f4e5e0ed8>, {29750: 1.0})), (('DET', 'NOUN', 'hyacinth'), defaultdict(<function <lambda> at 0x7f593e8dcc08>, {19777: 1.0})), (('ADV', 'VERB', 'look_like'), defaultdict(<function <lambda> at 0x7f4f4bbf9b18>, {454: 6.0})), (('DET', 'ADJ', 'nasty'), defaultdict(<function <lambda> at 0x7f5bf534ced8>, {17938: 2.0})), (('VERB', 'ADJ', 'multiple'), defaultdict(<function <lambda> at 0x7f4f3511ade8>, {6226: 1.0}))]\n",
      "\n",
      "TRIGRAMS (POS|curr lemma): [(('NOUN', '.', 'NOUN', 'automobile'), defaultdict(<function <lambda> at 0x7f4f38cbf398>, {2496: 1.0})), (('PRT', 'DET', 'ADJ', 'single'), defaultdict(<function <lambda> at 0x7f4f353ef488>, {333: 2.0})), (('DET', 'NOUN', 'NOUN', 'bull'), defaultdict(<function <lambda> at 0x7f5b4f31c488>, {13957: 1.0})), (('ADV', 'ADJ', 'NOUN', 'basis'), defaultdict(<function <lambda> at 0x7f4f330ce398>, {858: 2.0})), (('ADP', 'CONJ', 'VERB', 'consult'), defaultdict(<function <lambda> at 0x7f5bd6d60848>, {2598: 1.0}))]\n"
     ]
    }
   ],
   "source": [
    "def create_ngram_counts(root, keys_map, pos_grams=\"N\") :\n",
    "    #creates unigrams in format [(P_0)][word_sense] to counts\n",
    "    #creates bigrams in format [(P_1, P_0)][word sense] to counts\n",
    "    #creates trigrams in format [(P_2, P_1, P_0)][word sense] to counts\n",
    "    #where:\n",
    "    #P_2, P_1, and P_0 can be either:\n",
    "    # lemmas --> pos_grams=\"Y\"\n",
    "    # POS --> pos_grams=\"N\"\n",
    "    \n",
    "    #returns unigrams, bigrams, trigrams\n",
    "    \n",
    "    \"\"\"NOTE: Use an indexed keys map to return word sense indexes instead of word senses\"\"\"\n",
    "    \n",
    "    ### MODIFY TO and \"Y\" add reverse index to get word sense indexes instead\n",
    "    \"\"\"NOTE: n-grams are different than in lectures because WANT to use current context like POS and lemma;\n",
    "    not just preceding context. This is because we are trying to predict the current WORD SENSE, and already\n",
    "    have the current POS and lemma (due to Stanford NLP prediction used)\"\"\"\n",
    "    unigram_counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    bigram_counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    trigram_counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "\n",
    "    #iterate through docs\n",
    "    for i in root.getchildren():\n",
    "        \n",
    "        #iterate through sentences\n",
    "        #if at start of data, no p1/p2\n",
    "        p_1 = None\n",
    "        p_2 = None\n",
    "        instance_id = None\n",
    "        for j in i.getchildren():\n",
    "            #iterate through tags\n",
    "            for k in j.getchildren():\n",
    "                if pos_grams == \"Y\" :\n",
    "                    #use POS for bi/tri grams\n",
    "                    p_0 = k.attrib.get('pos')\n",
    "                else :\n",
    "                    #use lemma for bi/tri-grams\n",
    "                    p_0 = k.attrib.get('lemma')\n",
    "                if k.tag == 'instance':                \n",
    "                    instance_id = k.attrib.get('id')\n",
    "                    word_sense = keys_map[instance_id]\n",
    "                    if pos_grams == \"Y\" :\n",
    "                        lemma = k.attrib.get('lemma')\n",
    "                        #NOTE: Need to add lemmas here because need to limit to POS in just that lemma\n",
    "                        unigram_counts[(p_0,lemma)][word_sense] += 1\n",
    "                        bigram_counts[(p_1,p_0,lemma)][word_sense] += 1\n",
    "                        trigram_counts[(p_2,p_1,p_0,lemma)][word_sense] += 1\n",
    "                        \n",
    "                    else :\n",
    "                        unigram_counts[(p_0)][word_sense] += 1\n",
    "                        bigram_counts[(p_1,p_0)][word_sense] += 1\n",
    "                        trigram_counts[(p_2,p_1,p_0)][word_sense] += 1\n",
    "                #set lemmas = prior lemmas\n",
    "                p_2 = p_1\n",
    "                p_1 = p_0\n",
    "    \n",
    "    return unigram_counts, bigram_counts, trigram_counts\n",
    "\n",
    "#NOTE: using indexed version of maps\n",
    "train_unigrams, train_bigrams, train_trigrams = create_ngram_counts(\n",
    "    semcortrain_root, train_map_indexed, pos_grams=\"N\")\n",
    "\n",
    "train_unigrams_POS, train_bigrams_POS, train_trigrams_POS = create_ngram_counts(\n",
    "    semcortrain_root, train_map_indexed, pos_grams=\"Y\")\n",
    "\n",
    "print \"UNIGRAMS (lemmas):\", list(itertools.islice(train_unigrams.iteritems(), 0, 5))\n",
    "print\n",
    "print \"BIGRAMS (lemmas):\", list(itertools.islice(train_bigrams.iteritems(), 0, 5))\n",
    "print\n",
    "print \"TRIGRAMS (lemmas):\", list(itertools.islice(train_trigrams.iteritems(), 0, 5))\n",
    "print \n",
    "print \"UNIGRAMS (POS|curr lemma):\", list(itertools.islice(train_unigrams_POS.iteritems(), 0, 5))\n",
    "print\n",
    "print \"BIGRAMS (POS|curr lemma):\", list(itertools.islice(train_bigrams_POS.iteritems(), 0, 5))\n",
    "print\n",
    "print \"TRIGRAMS (POS|curr lemma):\", list(itertools.islice(train_trigrams_POS.iteritems(), 0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NORMALIZE CONTEXT COUNTS TO PROBABILITIES\n",
    "##### NOTE:\n",
    " - WE NEED TO CREATE \"BACKUPS\" FOR WHEN A CONTEXT|LEMMA OR A LEMMA HAS NOT BEEN SEEN IN TEST SET\n",
    "   - BACKUP FOR CONTEXT|LEMMA NOT SEEN IS EQUALLY WEIGHTED WORD SENSES FOR THAT LEMMA\n",
    "   - BACKUP FOR LEMMA NOT SEEN IS EQUALLY WEIGHTED *ALL* WORD SENSES (IN NEXT CODE BLOCK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NORMALIZED UNIGRAMS: [('fawn', {27399: 1.0}), ('xylem', {9287: 1.0}), ('unscientific', {15671: 1.0})]\n",
      "\n",
      "NORMALIZED BIGRAMS: [(('helium', 'temperature'), {221: 1.0}), (('little', 'note'), {3058: 1.0}), (('man', 'receive'), {142: 1.0})]\n",
      "\n",
      "NORMALIZED TRIGRAMS: [(('worthy', 'of', 'consideration'), {2292: 1.0}), (('at', 'the', 'beginning'), {6299: 0.375, 2269: 0.5, 4662: 0.125}), (('devout', ',', 'orthodox'), {10035: 1.0})]\n",
      "\n",
      "NORMALIZED UNIGRAMS (POS): [(('VERB', 'buck_up'), {33118: 1.0}), (('NOUN', 'starting_point'), {13345: 1.0}), (('VERB', 'breathe'), {2217: 0.8947368421052632, 28291: 0.05263157894736842, 29133: 0.05263157894736842})]\n",
      "\n",
      "NORMALIZED BIGRAMS (POS): [(('DET', 'NOUN', 'shock_therapy'), {29750: 1.0}), (('DET', 'NOUN', 'hyacinth'), {19777: 1.0}), (('ADV', 'VERB', 'look_like'), {454: 1.0})]\n",
      "\n",
      "NORMALIZED TRIGRAMS (POS): [(('NOUN', '.', 'NOUN', 'automobile'), {2496: 1.0}), (('DET', 'DET', 'ADJ', 'public'), {612: 1.0}), (('PRT', 'DET', 'ADJ', 'single'), {333: 1.0})]\n",
      "\n",
      "NORMALIZED UNIGRAMS (EQUAL PROBS, USED WHEN NEW CONTEXT SEEN FOR LEMMA): [('fawn', {27399: 1.0}), ('xylem', {9287: 1.0}), ('unscientific', {15671: 1.0}), ('writings', {26850: 1.0}), ('pathless', {18727: 1.0}), ('middle_west', {20692: 1.0}), ('absentminded', {10428: 1.0}), ('retain', {14256: 0.3333333333333333, 16830: 0.3333333333333333, 26527: 0.3333333333333333}), ('specific_heat', {32955: 1.0}), ('foul', {29676: 0.3333333333333333, 31540: 0.3333333333333333, 20367: 0.3333333333333333})]\n"
     ]
    }
   ],
   "source": [
    "### THIS SECTION TAKES COUNTS AND NORMALIZES TO PROBABILITIES (SUM to 1.0)\n",
    "\n",
    "def normalize_counter(c):\n",
    "    \"\"\"Given a dictionary of <item, counts>, return <item, fraction>.\"\"\"\n",
    "    total = sum(c.itervalues())\n",
    "    return {w:float(c[w])/total for w in c}\n",
    "\n",
    "def normalize_gram_counts(gram_counts_dict) :\n",
    "    probas = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    for context, ctr in gram_counts_dict.iteritems():\n",
    "        probas[context] = normalize_counter(ctr)\n",
    "    return probas\n",
    "\n",
    "def normalize_counter_equal_prob(c):\n",
    "    \"\"\"Given a dictionary of <item, counts>, return <item, fraction>.\"\"\"\n",
    "    average = 1/float(len(list(c.itervalues())))\n",
    "    return {w:average for w in c}\n",
    "\n",
    "def normalize_gram_counts_equal_prob(gram_counts_dict) :\n",
    "    probas = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    for context, ctr in gram_counts_dict.iteritems():\n",
    "        probas[context] = normalize_counter_equal_prob(ctr)\n",
    "    return probas\n",
    "\n",
    "\n",
    "#GET ALL NORMALIZED UNIGRAMS\n",
    "normalized_unigrams = normalize_gram_counts(train_unigrams)\n",
    "normalized_bigrams = normalize_gram_counts(train_bigrams)\n",
    "normalized_trigrams = normalize_gram_counts(train_trigrams)\n",
    "normalized_unigrams_POS = normalize_gram_counts(train_unigrams_POS)\n",
    "normalized_bigrams_POS = normalize_gram_counts(train_bigrams_POS)\n",
    "normalized_trigrams_POS = normalize_gram_counts(train_trigrams_POS)\n",
    "\n",
    "#GET NORMALIZED UNIGRAMS WITH EQUAL PROB - THIS BECOMES THE BACKUP FOR WHEN A CONTEXT ISN'T FOUND FOR A LEMMA\n",
    "normalized_unigrams_equal_prob = normalize_gram_counts_equal_prob(train_unigrams)\n",
    "\n",
    "\n",
    "print \"NORMALIZED UNIGRAMS:\", list(itertools.islice(normalized_unigrams.iteritems(), 0, 3))\n",
    "print \n",
    "print \"NORMALIZED BIGRAMS:\", list(itertools.islice(normalized_bigrams.iteritems(), 0, 3))\n",
    "print \n",
    "print \"NORMALIZED TRIGRAMS:\", list(itertools.islice(normalized_trigrams.iteritems(), 0, 3))\n",
    "print \n",
    "print \"NORMALIZED UNIGRAMS (POS):\", list(itertools.islice(normalized_unigrams_POS.iteritems(), 0, 3))\n",
    "print \n",
    "print \"NORMALIZED BIGRAMS (POS):\", list(itertools.islice(normalized_bigrams_POS.iteritems(), 0, 3))\n",
    "print \n",
    "print \"NORMALIZED TRIGRAMS (POS):\", list(itertools.islice(normalized_trigrams_POS.iteritems(), 0, 3))\n",
    "print \n",
    "print \"NORMALIZED UNIGRAMS (EQUAL PROBS, USED WHEN NEW CONTEXT SEEN FOR LEMMA):\", list(itertools.islice(normalized_unigrams_equal_prob.iteritems(), 0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE BACKUP PROBABILITIES (ASSIGNED TO *ALL* WORD SENSES) FOR WHEN AN UNSEEN LEMMA IS ENCOUNTERED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKUP INDEX PROBS (USED WHEN NEW LEMMA ENCOUNTERED): [(1, 3.018776791644026e-05), (2, 3.018776791644026e-05), (3, 3.018776791644026e-05), (4, 3.018776791644026e-05), (5, 3.018776791644026e-05)]\n",
      "\n",
      "BACKUP WORD SENSE PROBS (USED WHEN NEW LEMMA ENCOUNTERED): [('trumpet_vine%1:20:01::', 3.018776791644026e-05), ('movement%1:04:03::', 3.018776791644026e-05), ('scale%1:24:01::', 3.018776791644026e-05), ('b%1:05:00::', 3.018776791644026e-05), ('vacancy%1:26:00::', 3.018776791644026e-05)]\n"
     ]
    }
   ],
   "source": [
    "#create even probabilities for ALL Word senses (per training data); used when lemma never encountered before in test/eval\n",
    "\n",
    "def create_all_equal_probas(ws_index) :\n",
    "    #feed in training index to get all equal probability output (used when NEW LEMMA)\n",
    "    #do not include <unk> index prob\n",
    "    len_index = len(ws_index.keys())\n",
    "    backup_index_probs = dict()\n",
    "    backup_word_sense_probs = dict()\n",
    "    for index, word_sense in zip(ws_index.iterkeys(),ws_index.itervalues()) :\n",
    "        if index not in ('<unk>',0) :\n",
    "            backup_index_probs[index] = 1/float(len_index-1)\n",
    "            backup_word_sense_probs[word_sense] = 1/float(len_index-1)\n",
    "    return backup_index_probs, backup_word_sense_probs\n",
    "    \n",
    "backup_index_probs, backup_word_sense_probs = create_all_equal_probas(ws_index)\n",
    "\n",
    "print \"BACKUP INDEX PROBS (USED WHEN NEW LEMMA ENCOUNTERED):\", list(itertools.islice(backup_index_probs.iteritems(), 0, 5))\n",
    "print \n",
    "print \"BACKUP WORD SENSE PROBS (USED WHEN NEW LEMMA ENCOUNTERED):\", list(itertools.islice(backup_word_sense_probs.iteritems(), 0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE WORD SENSE PROBABILITY PREDICTOR (GIVEN CONTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tester output: 1st and 2nd print statements should be equivalent:\n",
      "\n",
      "1) WITH RELEVANT CONTEXT:\n",
      "{11281: 0.07142857142857142, 579: 0.6428571428571429, 5500: 0.14285714285714285, 2542: 0.14285714285714285}\n",
      "{11281: 0.07142857142857142, 579: 0.6428571428571429, 5500: 0.14285714285714285, 2542: 0.14285714285714285}\n",
      "\n",
      "2) NO RELEVANT CONTEXT; SHOULD RETURN AVERAGES\n",
      "{11281: 0.25, 579: 0.25, 5500: 0.25, 2542: 0.25}\n",
      "{11281: 0.25, 579: 0.25, 5500: 0.25, 2542: 0.25}\n",
      "\n",
      "3) WITH RELEVANT CONTEXT:\n",
      "{1: 1.0}\n",
      "{1: 1.0}\n",
      "\n",
      "4) WITH RELEVANT CONTEXT:\n",
      "{13776: 0.125, 14803: 0.125, 3341: 0.75}\n",
      "{13776: 0.125, 14803: 0.125, 3341: 0.75}\n",
      "\n",
      "5) NO RELEVANT LEMMA; SHOULD RETURN BACKUP (ALL) EQUAL WEIGHT\n",
      "6) WITH RELEVANT CONTEXT:\n",
      "{1489: 0.8, 6197: 0.2}\n",
      "{1489: 0.8, 6197: 0.2}\n"
     ]
    }
   ],
   "source": [
    "def ws_probas(lemma,context, n, normalized_ngrams, normalized_unigrams_equal_prob,backup_index,POS='N'):\n",
    "    ###NOTE: ALWAYS FEED IN normalized_unigrams with equal_prob (non-POS) as last argument\n",
    "    ###NOTE: USE THE PROPER BACKUP (index vs non-index) depending on what using\n",
    "    \"\"\"Compute p(each word sense | context)\"\"\"\n",
    "    #convert context as necessary\n",
    "        \n",
    "    #return possibilities if exists\n",
    "    if context in normalized_ngrams :\n",
    "        return normalized_ngrams[context]    \n",
    "    else :\n",
    "        #IF HAVE SEEN LEMMA, return an average across all word sense instances for that lemma\n",
    "        if lemma in normalized_unigrams_equal_prob :\n",
    "            return normalized_unigrams_equal_prob[lemma]\n",
    "        #IF HAVE NOT SEEN LEMMA, return an average across ALL WORD SENSES\n",
    "        else :\n",
    "            return backup_index\n",
    "\n",
    "###try a few instances\n",
    "#should return relevant context\n",
    "probas_tester_1 = ws_probas(lemma='art', context=('ADJ','NOUN', 'art'), n=2, \n",
    "          normalized_ngrams=normalized_bigrams_POS, \n",
    "          normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "          backup_index = backup_index_probs,\n",
    "          POS='Y')\n",
    "\n",
    "#should default to averages; no relevant context\n",
    "probas_tester_2 = ws_probas(lemma='art', context=('DET','DET', 'art'), n=2, \n",
    "          normalized_ngrams=normalized_bigrams_POS, \n",
    "          normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "          backup_index = backup_index_probs,\n",
    "          POS='Y')\n",
    "\n",
    "#should return relevant context\n",
    "probas_tester_3 = ws_probas(lemma='be', context=('like', 'eggs', 'be'), n=3, \n",
    "          normalized_ngrams=normalized_trigrams, \n",
    "          normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "          backup_index = backup_index_probs,\n",
    "          POS='N')\n",
    "\n",
    "#should return relevant context\n",
    "probas_tester_4 = ws_probas(lemma='iron', context=('iron'), n=1, \n",
    "          normalized_ngrams=normalized_unigrams, \n",
    "          normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "          backup_index = backup_index_probs,\n",
    "          POS='N')\n",
    "\n",
    "\n",
    "#should default backup_index; no relevant lemma (don't want <unk> or will skew)\n",
    "probas_tester_5 = ws_probas(lemma='fhqwhgads', context=('DET','DET','fhqwhgads'), n=2, \n",
    "          normalized_ngrams=normalized_bigrams_POS, \n",
    "          normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "          backup_index = backup_index_probs,\n",
    "          POS='Y')\n",
    "\n",
    "#should return relevant context\n",
    "probas_tester_6 = ws_probas(lemma='12', context=('12'), n=1, \n",
    "          normalized_ngrams=normalized_unigrams, \n",
    "          normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "          backup_index = backup_index_probs,\n",
    "          POS='N')\n",
    "\n",
    "print \"Tester output: 1st and 2nd print statements should be equivalent:\"\n",
    "print \n",
    "print \"1) WITH RELEVANT CONTEXT:\"\n",
    "print normalized_bigrams_POS[('ADJ','NOUN','art')]\n",
    "print probas_tester_1\n",
    "print\n",
    "print \"2) NO RELEVANT CONTEXT; SHOULD RETURN AVERAGES\"\n",
    "print normalized_unigrams_equal_prob[('art')]\n",
    "print probas_tester_2\n",
    "print\n",
    "print \"3) WITH RELEVANT CONTEXT:\"\n",
    "print normalized_trigrams[('like', 'eggs', 'be')]\n",
    "print probas_tester_3\n",
    "print\n",
    "print \"4) WITH RELEVANT CONTEXT:\"\n",
    "print normalized_unigrams[('iron')]\n",
    "print probas_tester_4\n",
    "print\n",
    "print \"5) NO RELEVANT LEMMA; SHOULD RETURN BACKUP (ALL) EQUAL WEIGHT\"\n",
    "##prints large matrix (assigns to ALL lemmas seen in training)\n",
    "# print normalized_unigrams_equal_prob[('art')]\n",
    "# print probas_tester_5\n",
    "\n",
    "\n",
    "print \"6) WITH RELEVANT CONTEXT:\"\n",
    "print normalized_unigrams[('12')]\n",
    "print probas_tester_6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITERATE THROUGH XML ROOT DATA (CAN BE TRAIN, TEST, EVAL) AND RETURN CONTEXTS FOR EACH INSTANCE ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cscaudill/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:27: DeprecationWarning:\n",
      "\n",
      "This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "\n",
      "/home/cscaudill/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:35: DeprecationWarning:\n",
      "\n",
      "This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "\n",
      "/home/cscaudill/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:37: DeprecationWarning:\n",
      "\n",
      "This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMS (lemmas): [('d032.s132.t004', 'iron'), ('d032.s132.t005', 'tire'), ('d032.s132.t006', 'grip'), ('d032.s132.t007', 'wheel'), ('d032.s132.t000', 'spoke')]\n",
      "\n",
      "BIGRAMS (lemmas): [('d032.s132.t004', ('the', 'iron')), ('d032.s132.t005', ('iron', 'tire')), ('d032.s132.t006', ('tire', 'grip')), ('d032.s132.t007', ('the', 'wheel')), ('d032.s132.t000', ('the', 'spoke'))]\n",
      "\n",
      "TRIGRAMS (lemmas): [('d032.s132.t004', (',', 'the', 'iron')), ('d032.s132.t005', ('the', 'iron', 'tire')), ('d032.s132.t006', ('iron', 'tire', 'grip')), ('d032.s132.t007', ('onto', 'the', 'wheel')), ('d032.s132.t000', ('.', 'the', 'spoke'))]\n",
      "\n",
      "UNIGRAMS (POS|curr lemma): [('d032.s132.t004', ('NOUN', 'iron')), ('d032.s132.t005', ('NOUN', 'tire')), ('d032.s132.t006', ('VERB', 'grip')), ('d032.s132.t007', ('NOUN', 'wheel')), ('d032.s132.t000', ('NOUN', 'spoke'))]\n",
      "\n",
      "BIGRAMS (POS|curr lemma): [('d032.s132.t004', ('DET', 'NOUN', 'iron')), ('d032.s132.t005', ('NOUN', 'NOUN', 'tire')), ('d032.s132.t006', ('NOUN', 'VERB', 'grip')), ('d032.s132.t007', ('DET', 'NOUN', 'wheel')), ('d032.s132.t000', ('DET', 'NOUN', 'spoke'))]\n",
      "\n",
      "TRIGRAMS (POS|curr lemma): [('d032.s132.t004', ('.', 'DET', 'NOUN', 'iron')), ('d032.s132.t005', ('DET', 'NOUN', 'NOUN', 'tire')), ('d032.s132.t006', ('NOUN', 'NOUN', 'VERB', 'grip')), ('d032.s132.t007', ('ADP', 'DET', 'NOUN', 'wheel')), ('d032.s132.t000', ('.', 'DET', 'NOUN', 'spoke'))]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"MODIFIED 8-20-17\"\"\"\n",
    "\n",
    "### CREATE CONTEXT DICT BY INSTANCE NUMBER AS ITERATE THROUGH A DOCUMENT\n",
    "\n",
    "def create_ngram_contexts(root, pos_grams=\"N\") :\n",
    "    #creates unigrams in format instance to (P_0, lemma/POS)\n",
    "    #creates bigrams in format instance to (P_1, P_0, lemma/POS)\n",
    "    #creates trigrams in format instance to (P_2, P_1, P_0, lemma/POS)\n",
    "    #where:\n",
    "    #P_2 P_1 P_0 can be either:\n",
    "    # lemmas --> pos_grams=\"Y\"\n",
    "    # POS --> pos_grams=\"N\"\n",
    "    \n",
    "    #returns unigram bigrams and trigram contexts\n",
    "    \n",
    "    \"\"\"NOTE: Use an indexed keys map to return word sense indexes instead of word senses\"\"\"\n",
    "    \n",
    "    ### MODIFY TO and \"Y\" add reverse index to get word sense indexes instead\n",
    "    \"\"\"NOTE: n-grams are different than in lectures because WANT to use current context like POS and lemma;\n",
    "    not just preceding context. This is because we are trying to predict the current WORD SENSE, and already\n",
    "    have the current POS and lemma (due to Stanford NLP prediction used)\"\"\"\n",
    "    unigram_contexts = defaultdict(dict)\n",
    "    bigram_contexts = defaultdict(dict)\n",
    "    trigram_contexts = defaultdict(dict)\n",
    "\n",
    "    #iterate through docs\n",
    "    for i in root.getchildren():\n",
    "#     for i in root.getchildren()[0:2]:\n",
    "        \n",
    "        #iterate through sentences\n",
    "        #if at start of data, no p1/p2\n",
    "        p_1 = None\n",
    "        p_2 = None\n",
    "        instance_id = None\n",
    "        for j in i.getchildren():\n",
    "            #iterate through tags\n",
    "            for k in j.getchildren():\n",
    "                if pos_grams == \"Y\" :\n",
    "                    #use POS for bi/tri grams\n",
    "                    p_0 = k.attrib.get('pos')\n",
    "                else :\n",
    "                    #use lemma for bi/tri-grams\n",
    "                    p_0 = k.attrib.get('lemma')\n",
    "                if k.tag == 'instance':                \n",
    "                    instance_id = k.attrib.get('id')\n",
    "                    if pos_grams == \"Y\" :\n",
    "                        lemma = k.attrib.get('lemma')\n",
    "                        #NOTE: Need to add lemmas here because need to limit to POS in just that lemma\n",
    "                        unigram_contexts[instance_id] = (p_0,lemma)\n",
    "                        bigram_contexts[instance_id] = (p_1,p_0,lemma)\n",
    "                        trigram_contexts[instance_id] = (p_2,p_1,p_0,lemma)\n",
    "                        \n",
    "                    else :\n",
    "                        unigram_contexts[instance_id] = (p_0)\n",
    "                        bigram_contexts[instance_id] = (p_1,p_0)\n",
    "                        trigram_contexts[instance_id] = (p_2,p_1,p_0)\n",
    "                #set lemmas = prior lemmas\n",
    "                p_2 = p_1\n",
    "                p_1 = p_0\n",
    "    \n",
    "    return unigram_contexts, bigram_contexts, trigram_contexts\n",
    "\n",
    "\n",
    "### TRAIN VERSION ###\n",
    "#NOTE: using indexed version of maps\n",
    "train_context_unigrams, train_context_bigrams, train_context_trigrams = create_ngram_contexts(\n",
    "    semcortrain_root, pos_grams=\"N\")\n",
    "\n",
    "train_context_unigrams_POS, train_context_bigrams_POS, train_context_trigrams_POS = create_ngram_contexts(\n",
    "    semcortrain_root, pos_grams=\"Y\")\n",
    "\n",
    "\n",
    "\"\"\"8_20_17_EVAL\"\"\"\n",
    "#--> Change semcortestroot references to Eval\n",
    "\n",
    "### TEST VERSION ###\n",
    "#NOTE: using indexed version of maps\n",
    "test_context_unigrams, test_context_bigrams, test_context_trigrams = create_ngram_contexts(\n",
    "    var_test_root, pos_grams=\"N\")\n",
    "\n",
    "test_context_unigrams_POS, test_context_bigrams_POS, test_context_trigrams_POS = create_ngram_contexts(\n",
    "    var_test_root, pos_grams=\"Y\")\n",
    "\n",
    "\n",
    "print \"UNIGRAMS (lemmas):\", list(itertools.islice(train_context_unigrams.iteritems(), 0, 5))\n",
    "print\n",
    "print \"BIGRAMS (lemmas):\", list(itertools.islice(train_context_bigrams.iteritems(), 0, 5))\n",
    "print\n",
    "print \"TRIGRAMS (lemmas):\", list(itertools.islice(train_context_trigrams.iteritems(), 0, 5))\n",
    "print \n",
    "print \"UNIGRAMS (POS|curr lemma):\", list(itertools.islice(train_context_unigrams_POS.iteritems(), 0, 5))\n",
    "print\n",
    "print \"BIGRAMS (POS|curr lemma):\", list(itertools.islice(train_context_bigrams_POS.iteritems(), 0, 5))\n",
    "print\n",
    "print \"TRIGRAMS (POS|curr lemma):\", list(itertools.islice(train_context_trigrams_POS.iteritems(), 0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE DENSE DICT OF MODEL PREDICTIONS\n",
    " - Note: Be careful with all variables to ensure correct info gets fed to each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAM PRED (MFS): [('d032.s132.t004', {13776: 0.125, 14803: 0.125, 3341: 0.75}), ('d032.s132.t005', {11920: 0.21428571428571427, 4825: 0.5714285714285714, 10970: 0.21428571428571427}), ('d032.s132.t006', {4452: 0.4090909090909091, 5255: 0.3181818181818182, 18312: 0.09090909090909091, 31020: 0.045454545454545456, 27920: 0.045454545454545456, 15956: 0.09090909090909091})]\n",
      "\n",
      "BIGRAM PRED: [('d032.s132.t004', {3341: 1.0}), ('d032.s132.t005', {4825: 1.0}), ('d032.s132.t006', {4452: 1.0})]\n",
      "\n",
      "TRIGRAM PRED: [('d032.s132.t004', {3341: 1.0}), ('d032.s132.t005', {4825: 1.0}), ('d032.s132.t006', {4452: 1.0})]\n",
      "\n",
      "UNIGRAM PRED (POS): [('d032.s132.t004', {14803: 0.14285714285714285, 3341: 0.8571428571428571}), ('d032.s132.t005', {4825: 1.0}), ('d032.s132.t006', {18312: 0.15384615384615385, 4452: 0.6923076923076923, 15956: 0.15384615384615385})]\n",
      "\n",
      "BIGRAM PRED (POS): [('d032.s132.t004', {3341: 1.0}), ('d032.s132.t005', {4825: 1.0}), ('d032.s132.t006', {4452: 0.8, 15956: 0.2})]\n",
      "\n",
      "TRIGRAM PRED (POS): [('d032.s132.t004', {3341: 1.0}), ('d032.s132.t005', {4825: 1.0}), ('d032.s132.t006', {4452: 1.0})]\n",
      "\n",
      "{417: 0.10627177700348432, 524: 0.09059233449477352, 1172: 0.050522648083623695, 20: 0.7421602787456446, 17821: 0.003484320557491289, 9023: 0.006968641114982578}\n"
     ]
    }
   ],
   "source": [
    "def create_predictions_dict(context_dict, \n",
    "                             n, \n",
    "                             normalized_ngrams, \n",
    "                             normalized_unigrams_equal_prob,\n",
    "                             backup_index,\n",
    "                             POS='N') :\n",
    "    \n",
    "    predictions_dict = defaultdict(dict)\n",
    "    counter = 0\n",
    "    for instance_id, context in context_dict.iteritems() :\n",
    "        if type(context) is str :\n",
    "            #for context of only single lemma; doesn't come in tuple (comes in string); last entry would return single letter\n",
    "            lemma = context\n",
    "        else :\n",
    "            #when comes in as a tuple, can take the last entry as the lemma\n",
    "            lemma = context[-1]\n",
    "        predictions = ws_probas(lemma, context, n, normalized_ngrams, normalized_unigrams_equal_prob, backup_index, POS)\n",
    "        predictions_dict[instance_id] = predictions\n",
    "        \n",
    "    return predictions_dict\n",
    "\n",
    "\"\"\"\n",
    "### WILL DIFFER:\n",
    " - context_dict (can be train, test, POS, n_gram, etc)\n",
    " - n\n",
    " - normalized_ngrams (train ONLY, can be POS, n_gram, etc)\n",
    " - POS\n",
    " \n",
    "### WILL BE CONSISTENT: \n",
    " - normalized_unigrams_equal_prob\n",
    " - backup index\n",
    "\"\"\"\n",
    "\n",
    "### TRAIN ###\n",
    "unigram_pred_train_dense = create_predictions_dict(context_dict=train_context_unigrams,\n",
    "                                                   n=1,\n",
    "                                                   normalized_ngrams=normalized_unigrams,\n",
    "                                                   normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "                                                   backup_index = backup_index_probs,\n",
    "                                                   POS='N')\n",
    "bigram_pred_train_dense = create_predictions_dict(context_dict=train_context_bigrams,\n",
    "                                                   n=2,\n",
    "                                                   normalized_ngrams=normalized_bigrams,\n",
    "                                                   normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "                                                   backup_index = backup_index_probs,\n",
    "                                                   POS='N')\n",
    "trigram_pred_train_dense = create_predictions_dict(context_dict=train_context_trigrams,\n",
    "                                                   n=3,\n",
    "                                                   normalized_ngrams=normalized_trigrams,\n",
    "                                                   normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "                                                   backup_index = backup_index_probs,\n",
    "                                                   POS='N')\n",
    "unigram_POS_pred_train_dense = create_predictions_dict(context_dict=train_context_unigrams_POS,\n",
    "                                                   n=1,\n",
    "                                                   normalized_ngrams=normalized_unigrams_POS,\n",
    "                                                   normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "                                                   backup_index = backup_index_probs,\n",
    "                                                   POS='N')\n",
    "bigram_POS_pred_train_dense = create_predictions_dict(context_dict=train_context_bigrams_POS,\n",
    "                                                   n=2,\n",
    "                                                   normalized_ngrams=normalized_bigrams_POS,\n",
    "                                                   normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "                                                   backup_index = backup_index_probs,\n",
    "                                                   POS='N')\n",
    "trigram_POS_pred_train_dense = create_predictions_dict(context_dict=train_context_trigrams_POS,\n",
    "                                                   n=3,\n",
    "                                                   normalized_ngrams=normalized_trigrams_POS,\n",
    "                                                   normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "                                                   backup_index = backup_index_probs,\n",
    "                                                   POS='N')\n",
    "\n",
    "### TEST ###\n",
    "unigram_pred_test_dense = create_predictions_dict(context_dict=test_context_unigrams,\n",
    "                                                   n=1,\n",
    "                                                   normalized_ngrams=normalized_unigrams,\n",
    "                                                   normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "                                                   backup_index = backup_index_probs,\n",
    "                                                   POS='N')\n",
    "bigram_pred_test_dense = create_predictions_dict(context_dict=test_context_bigrams,\n",
    "                                                   n=2,\n",
    "                                                   normalized_ngrams=normalized_bigrams,\n",
    "                                                   normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "                                                   backup_index = backup_index_probs,\n",
    "                                                   POS='N')\n",
    "trigram_pred_test_dense = create_predictions_dict(context_dict=test_context_trigrams,\n",
    "                                                   n=3,\n",
    "                                                   normalized_ngrams=normalized_trigrams,\n",
    "                                                   normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "                                                   backup_index = backup_index_probs,\n",
    "                                                   POS='N')\n",
    "unigram_POS_pred_test_dense = create_predictions_dict(context_dict=test_context_unigrams_POS,\n",
    "                                                   n=1,\n",
    "                                                   normalized_ngrams=normalized_unigrams_POS,\n",
    "                                                   normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "                                                   backup_index = backup_index_probs,\n",
    "                                                   POS='N')\n",
    "bigram_POS_pred_test_dense = create_predictions_dict(context_dict=test_context_bigrams_POS,\n",
    "                                                   n=2,\n",
    "                                                   normalized_ngrams=normalized_bigrams_POS,\n",
    "                                                   normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "                                                   backup_index = backup_index_probs,\n",
    "                                                   POS='N')\n",
    "trigram_POS_pred_test_dense = create_predictions_dict(context_dict=test_context_trigrams_POS,\n",
    "                                                   n=3,\n",
    "                                                   normalized_ngrams=normalized_trigrams_POS,\n",
    "                                                   normalized_unigrams_equal_prob=normalized_unigrams_equal_prob,\n",
    "                                                   backup_index = backup_index_probs,\n",
    "                                                   POS='N')\n",
    "\n",
    "\n",
    "\n",
    "print \"UNIGRAM PRED (MFS):\", list(itertools.islice(unigram_pred_train_dense.iteritems(), 0, 3))\n",
    "print\n",
    "print \"BIGRAM PRED:\", list(itertools.islice(bigram_pred_train_dense.iteritems(), 0, 3))\n",
    "print    \n",
    "print \"TRIGRAM PRED:\", list(itertools.islice(trigram_pred_train_dense.iteritems(), 0, 3))\n",
    "print    \n",
    "print \"UNIGRAM PRED (POS):\", list(itertools.islice(unigram_POS_pred_train_dense.iteritems(), 0, 3))\n",
    "print\n",
    "print \"BIGRAM PRED (POS):\", list(itertools.islice(bigram_POS_pred_train_dense.iteritems(), 0, 3))\n",
    "print    \n",
    "print \"TRIGRAM PRED (POS):\", list(itertools.islice(trigram_POS_pred_train_dense.iteritems(), 0, 3))\n",
    "print  \n",
    "\n",
    "print unigram_pred_test_dense['d000.s000.t000']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN INDIVIDUAL MODELS AND GET STATS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE MODEL STATS PRINTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAM LEMMA (MFS MODEL): TEST DATA\n",
      "NUM SAMPLES:\t\t\t1850\n",
      "CORRECT PREDS:\t\t\t1008 /    54.5%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t778.6 /    42.1%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t894.4 /    48.3%\n",
      "\n",
      "BIGRAM LEMMA: TEST DATA\n",
      "NUM SAMPLES:\t\t\t1850\n",
      "CORRECT PREDS:\t\t\t1041 /    56.3%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t751.6 /    40.6%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t854.9 /    46.2%\n",
      "\n",
      "TRIGRAM LEMMA: TEST DATA\n",
      "NUM SAMPLES:\t\t\t1850\n",
      "CORRECT PREDS:\t\t\t1015 /    54.9%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t598.4 /    32.3%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t715.3 /    38.7%\n",
      "\n",
      "UNIGRAM POS: TEST DATA\n",
      "NUM SAMPLES:\t\t\t1850\n",
      "CORRECT PREDS:\t\t\t1136 /    61.4%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t913.5 /    49.4%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t1006.9 /    54.4%\n",
      "\n",
      "BIGRAM POS: TEST DATA\n",
      "NUM SAMPLES:\t\t\t1850\n",
      "CORRECT PREDS:\t\t\t1119 /    60.5%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t928.1 /    50.2%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t1013.8 /    54.8%\n",
      "\n",
      "TRIGRAM POS: TEST DATA\n",
      "NUM SAMPLES:\t\t\t1850\n",
      "CORRECT PREDS:\t\t\t1057 /    57.1%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t840.2 /    45.4%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t936.3 /    50.6%\n",
      "\n",
      "LSTM STATS: TEST DATA\n",
      "NUM SAMPLES:\t\t\t1850\n",
      "CORRECT PREDS:\t\t\t612 /    33.1%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t510.3 /    27.6%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t605.9 /    32.8%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "def print_model_stats(pred_dense, ids_array, max_samples = None, voting_classifier=None) :\n",
    "    #function to iterate through to see how many times the highest prob prediction is correct\n",
    "    #also gives prop\n",
    "    \n",
    "    if max_samples is not None :\n",
    "        num_samples = max_samples\n",
    "    else :\n",
    "        num_samples = len(ids_array)\n",
    "\n",
    "        \n",
    "    counter = 0\n",
    "    correct_pred = 0                      #binary 1/0 credit given\n",
    "    correct_pred_weight = 0              #if max prob predicts correctly, give max prob\n",
    "    weight_correct_sense = 0             #prob assigned to correct sense, whether max or not\n",
    "\n",
    "\n",
    "    for (instance_id, preds), act_index in zip(sorted(pred_dense.iteritems()), ids_array) :\n",
    "        #assign relevant scoring if correct or if correct word sense within predictions\n",
    "        #note: pred_dense must be sorted to ensure comes in same order as ids_array\n",
    "        if act_index == 0 or preds is None:\n",
    "            #if <unk> (index=0), give no credit. Saves time because does not iterate through a full/large dense array of predictions.\n",
    "            #preds comes in None from voting classifier when encountering act_index==0\n",
    "            correct_pred += 0\n",
    "            correct_pred_weight += 0\n",
    "        else :\n",
    "            #get maximum probability (sorted makes default to first index if there is a tie)\n",
    "            max_index = max(sorted(preds.iteritems()), key=operator.itemgetter(1))[0]\n",
    "            max_prob = max(sorted(preds.iteritems()), key=operator.itemgetter(1))[1]\n",
    "            if max_index == act_index :\n",
    "                #correct maximum\n",
    "                correct_pred += 1\n",
    "                correct_pred_weight += max_prob\n",
    "                weight_correct_sense += preds[act_index]\n",
    "            elif act_index in preds.keys() :\n",
    "                #not correct but probability was assigned\n",
    "                weight_correct_sense += preds[act_index]\n",
    "                \n",
    "        counter += 1\n",
    "        if counter >= max_samples and max_samples is not None:\n",
    "            break\n",
    "\n",
    "    correct_pred_rate = float(correct_pred)/num_samples\n",
    "    correct_pred_weight_rate = float(correct_pred_weight)/num_samples\n",
    "    weight_correct_sense_rate = float(weight_correct_sense)/num_samples\n",
    "        \n",
    "    print \"NUM SAMPLES:\\t\\t\\t\", num_samples\n",
    "    print \"CORRECT PREDS:\\t\\t\\t\", correct_pred, \"/   \", '{:.1%}'.format(correct_pred_rate)\n",
    "    print \"CORRECT PRED WEIGHT ASSIGNED:\\t\", \"{:0.1f}\".format(correct_pred_weight), \"/   \", '{:.1%}'.format(correct_pred_weight_rate) \n",
    "    print \"TOTAL WEIGHT TO CORRECT SENSE:\\t\", \"{:0.1f}\".format(weight_correct_sense), \"/   \", '{:.1%}'.format(weight_correct_sense_rate)\n",
    "\n",
    "print \"UNIGRAM LEMMA (MFS MODEL): TEST DATA\"\n",
    "print_model_stats(unigram_pred_test_dense, var_test_ids, max_samples = None)\n",
    "print\n",
    "print \"BIGRAM LEMMA: TEST DATA\"\n",
    "print_model_stats(bigram_pred_test_dense, var_test_ids, max_samples = None)\n",
    "print \n",
    "print \"TRIGRAM LEMMA: TEST DATA\"\n",
    "print_model_stats(trigram_pred_test_dense, var_test_ids, max_samples = None)\n",
    "print\n",
    "print \"UNIGRAM POS: TEST DATA\"\n",
    "print_model_stats(unigram_POS_pred_test_dense, var_test_ids, max_samples = None)\n",
    "print\n",
    "print \"BIGRAM POS: TEST DATA\"\n",
    "print_model_stats(bigram_POS_pred_test_dense, var_test_ids, max_samples = None)\n",
    "print \n",
    "print \"TRIGRAM POS: TEST DATA\"\n",
    "print_model_stats(trigram_POS_pred_test_dense, var_test_ids, max_samples = None)\n",
    "print\n",
    "#edited 8_19_17_KO\n",
    "print \"LSTM STATS: TEST DATA\"\n",
    "print_model_stats(lstm_dense_preds_test_matched_index, var_test_ids, max_samples = None)\n",
    "print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE MODEL STATS RETURNER (SAME EXACT FUNCTION AS PRINT_MODEL_STATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def return_model_stats(pred_dense, ids_array, max_samples = None, voting_classifier=None) :\n",
    "    #function to iterate through to see how many times the highest prob prediction is correct\n",
    "    #also gives prop\n",
    "    \n",
    "    if max_samples is not None :\n",
    "        num_samples = max_samples\n",
    "    else :\n",
    "        num_samples = len(ids_array)\n",
    "\n",
    "        \n",
    "    counter = 0\n",
    "    correct_pred = 0                      #binary 1/0 credit given\n",
    "    correct_pred_weight = 0              #if max prob predicts correctly, give max prob\n",
    "    weight_correct_sense = 0             #prob assigned to correct sense, whether max or not\n",
    "\n",
    "\n",
    "    for (instance_id, preds), act_index in zip(sorted(pred_dense.iteritems()), ids_array) :\n",
    "        #assign relevant scoring if correct or if correct word sense within predictions\n",
    "        #note: pred_dense must be sorted to ensure comes in same order as ids_array\n",
    "        if act_index == 0 or preds is None:\n",
    "            #if <unk> (index=0), give no credit. Saves time because does not iterate through a full/large dense array of predictions.\n",
    "            #preds comes in None from voting classifier when encountering act_index==0\n",
    "            correct_pred += 0\n",
    "            correct_pred_weight += 0\n",
    "        else :\n",
    "            #get maximum probability (sorted makes default to first index if there is a tie)\n",
    "            max_index = max(sorted(preds.iteritems()), key=operator.itemgetter(1))[0]\n",
    "            max_prob = max(sorted(preds.iteritems()), key=operator.itemgetter(1))[1]\n",
    "            if max_index == act_index :\n",
    "                #correct maximum\n",
    "                correct_pred += 1\n",
    "                correct_pred_weight += max_prob\n",
    "                weight_correct_sense += preds[act_index]\n",
    "            elif act_index in preds.keys() :\n",
    "                #not correct but probability was assigned\n",
    "                weight_correct_sense += preds[act_index]\n",
    "                \n",
    "        counter += 1\n",
    "        if counter >= max_samples and max_samples is not None:\n",
    "            break\n",
    "\n",
    "    correct_pred_rate = float(correct_pred)/num_samples\n",
    "    correct_pred_weight_rate = float(correct_pred_weight)/num_samples\n",
    "    weight_correct_sense_rate = float(weight_correct_sense)/num_samples\n",
    "    \n",
    "    return num_samples, correct_pred, correct_pred_weight, weight_correct_sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VOTING CLASSIFIER FUNCTION (BOTH HARD AND SOFT CLASSIFIERS RETURNED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HARD VOTING CLASSIFIER STATS:\n",
      "NUM SAMPLES:\t\t\t1850\n",
      "CORRECT PREDS:\t\t\t1008 /    54.5%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t1008.0 /    54.5%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t1072.0 /    57.9%\n",
      "\n",
      "SOFT VOTING CLASSIFIER STATS:\n",
      "NUM SAMPLES:\t\t\t1850\n",
      "CORRECT PREDS:\t\t\t1136 /    61.4%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t872.1 /    47.1%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t961.9 /    52.0%\n"
     ]
    }
   ],
   "source": [
    "def voting_classifier_dense_preds(pred_dense_model_list, ids_array, weights_list=None, max_samples=None) :\n",
    "\n",
    "    \"\"\"\n",
    "    Takes in:\n",
    "    pred_dense_model_list: model predictions in dense format\n",
    "    ids_array: corresponding train or test id actual word senses (by index)\n",
    "    weights_list (optional): Optional weights list for re-weighting in SOFT CLASSIFIER ONLY. None returns even weighting.\n",
    "    max_samples (optional): Stop each model after a certain amount of instance_ids. None returns all instance_ids\n",
    "    \n",
    "    RETURNS\n",
    "    vote_counts -> Dense dictionary by instance_id adding 1/num_models to each max sense predicted per model\n",
    "    weight_counts -> Dense dictionary by instance_id adding weight to any sense predicted per model\n",
    "    \n",
    "    NOTE: returns in same dense dictionary format that we can run through print_model_stats() for evaluation\n",
    "    vote_counts -> Used for HARD VOTING CLASSIFIER EVAL\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    num_models = len(pred_dense_model_list)\n",
    "    \n",
    "    if weights_list is None :\n",
    "        #defaults to even weighting\n",
    "        weights_list = [1/float(num_models) for m in pred_dense_model_list]\n",
    "    elif sum(weights_list) > 1.01 or sum(weights_list) < .99:\n",
    "        print \"RE-DO: WEIGHTS DO NOT SUM TO 1.00\"\n",
    "    \n",
    "    if max_samples is not None :\n",
    "        num_samples = max_samples\n",
    "    else :\n",
    "        num_samples = len(ids_array)\n",
    "    \n",
    "    \n",
    "    vote_counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    weight_counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    \n",
    "    model_counter = 0    \n",
    "    for model_preds in pred_dense_model_list :\n",
    "        #get key info from each predictive model output, append to a default_dict to be referenced later\n",
    "        \n",
    "        counter = 0\n",
    "        for (instance_id, preds), act_index in zip(sorted(model_preds.iteritems()), ids_array) :            \n",
    "            \n",
    "            #KO added 8_19_17_KO\n",
    "            if preds is None :\n",
    "                #handle None from LSTM\n",
    "                continue\n",
    "            \n",
    "            #assign relevant scoring if correct or if correct word sense within predictions\n",
    "            #note: pred_dense must be sorted to ensure comes in same order as ids_array\n",
    "            if act_index == 0 :\n",
    "                #if unk, do nothing. No model will get correct.\n",
    "                #assigning to none will allow to filter out when printing stats\n",
    "                vote_counts[instance_id] = None\n",
    "                \n",
    "                weight_counts[instance_id][act_index] = None                \n",
    "                #ORIG\n",
    "#                 weight_counts[instance_id][index] = None\n",
    "            \n",
    "            else :\n",
    "                #get maximum probability (sorted makes default to first index if there is a tie)\n",
    "                max_index = max(sorted(preds.iteritems()), key=operator.itemgetter(1))[0]\n",
    "                max_prob = max(sorted(preds.iteritems()), key=operator.itemgetter(1))[1]\n",
    "                vote_counts[instance_id][max_index] += 1/float(num_models)\n",
    "                #assign a weighted amount for each prediction within model for instance id\n",
    "                for index, prob in preds.iteritems() :\n",
    "                    weight_counts[instance_id][index] += prob*weights_list[model_counter] #picks up corresponding weight for model\n",
    "                            \n",
    "            counter += 1\n",
    "            if counter >= max_samples and max_samples is not None:\n",
    "                break\n",
    "                \n",
    "        model_counter += 1\n",
    "    \n",
    "    return vote_counts, weight_counts\n",
    "\n",
    "model_list = [unigram_pred_test_dense,unigram_POS_pred_test_dense]\n",
    "model_weights = [0.4, 0.6]\n",
    "max_samples = None\n",
    "\n",
    "hard, soft = voting_classifier_dense_preds(pred_dense_model_list=model_list, ids_array=var_test_ids,\n",
    "                                     weights_list=model_weights, max_samples=max_samples)\n",
    "\n",
    "#KO MODIFIED TO 50 SAMPLES\n",
    "print \"HARD VOTING CLASSIFIER STATS:\"\n",
    "print_model_stats(hard, var_test_ids, max_samples=None)\n",
    "print \n",
    "print \"SOFT VOTING CLASSIFIER STATS:\"\n",
    "print_model_stats(soft, var_test_ids, max_samples=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENT GRID SEARCH FOR OPTIMAL ENSEMBLES (EVEN WEIGHTING)\n",
    "NOTE: Only use to find optimal models in test set; not in Eval set. Must reuse optimal models from test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-471-e256887f3ec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m ensemble_data = grid_search_ensembles(model_map=model_map,model_name_map=model_name_map,\n\u001b[0;32m---> 78\u001b[0;31m                                          correct_labels=var_test_ids, max_samples=None)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-471-e256887f3ec4>\u001b[0m in \u001b[0;36mgrid_search_ensembles\u001b[0;34m(model_map, model_name_map, correct_labels, max_samples)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m#load into hard and soft classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         hard, soft = voting_classifier_dense_preds(pred_dense_model_list=model_list, ids_array=correct_labels,\n\u001b[0;32m---> 31\u001b[0;31m                                          weights_list=None, max_samples=max_samples)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#return hard classifier output, add to ensemble_tracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-452-c058c4cf8eda>\u001b[0m in \u001b[0;36mvoting_classifier_dense_preds\u001b[0;34m(pred_dense_model_list, ids_array, weights_list, max_samples)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;31m#get maximum probability (sorted makes default to first index if there is a tie)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mmax_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0mmax_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mvote_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minstance_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"CURRENTLY USING TEST_IDS, MAX_SAMPLES=NONE, ETC... WILL WANT TO STANDARDIZE INTO A FUNCTION\"\"\"\n",
    "\"\"\"ONCE IN FUNCTION, ADD AN OPTIMAL WEIGHTS FUNCTION \"\"\"\n",
    "\n",
    "def grid_search_ensembles(model_map, model_name_map, correct_labels, max_samples=None) :\n",
    "    \n",
    "    \"\"\"FUNCTION RETURNING KEY STATISTICS FOR EVERY MODEL COMBINATION AFTER RUNNING EACH MODEL ENSEMBLE\n",
    "    THROUGH HARD AND SOFT VOTING CLASSIFIERS    \n",
    "    \"\"\"\n",
    "\n",
    "    #will add combo numbers to a combos_list\n",
    "    combos_list = []\n",
    "\n",
    "    #get combo numbers\n",
    "    for num in xrange(1, len(model_map.keys())):\n",
    "        #note: not 0 because 0 returns combo of length zero, which we don't want\n",
    "        for combo in itertools.combinations(model_map.keys(), num+1):\n",
    "            combos_list.append(combo)\n",
    "    \n",
    "    #to be used to append names and stats to\n",
    "    ensemble_tracker = []\n",
    "\n",
    "\n",
    "    #iterate through each set of combo numbers corresponding to a model\n",
    "    for model_nums in combos_list:\n",
    "        #get models and names associated\n",
    "        model_list = [model_map[num] for num in model_nums]\n",
    "        model_names_list = [model_name_map[num] for num in model_nums]\n",
    "\n",
    "        #load into hard and soft classifier\n",
    "        hard, soft = voting_classifier_dense_preds(pred_dense_model_list=model_list, ids_array=correct_labels,\n",
    "                                         weights_list=None, max_samples=max_samples)\n",
    "\n",
    "        #return hard classifier output, add to ensemble_tracker\n",
    "        num_samples, correct_pred, correct_pred_weight, weight_correct_sense = return_model_stats(\n",
    "            hard, correct_labels, max_samples=max_samples)\n",
    "\n",
    "        correct_pred_rate = float(correct_pred)/num_samples\n",
    "        correct_pred_weight_rate = float(correct_pred_weight)/num_samples\n",
    "        weight_correct_sense_rate = float(weight_correct_sense)/num_samples\n",
    "\n",
    "\n",
    "        ensemble_tracker.append((model_names_list, model_nums, \"HARD\", \n",
    "                                num_samples, correct_pred, correct_pred_weight, weight_correct_sense,\n",
    "                               correct_pred_rate,correct_pred_weight_rate,weight_correct_sense_rate))\n",
    "\n",
    "\n",
    "        #return soft classifier output, add to ensemble_tracker\n",
    "        num_samples, correct_pred, correct_pred_weight, weight_correct_sense = return_model_stats(\n",
    "            soft, correct_labels, max_samples=max_samples)\n",
    "\n",
    "        correct_pred_rate = float(correct_pred)/num_samples\n",
    "        correct_pred_weight_rate = float(correct_pred_weight)/num_samples\n",
    "        weight_correct_sense_rate = float(weight_correct_sense)/num_samples\n",
    "\n",
    "\n",
    "        ensemble_tracker.append((model_names_list, model_nums, \"SOFT\", \n",
    "                                num_samples, correct_pred, correct_pred_weight, weight_correct_sense,\n",
    "                               correct_pred_rate,correct_pred_weight_rate,weight_correct_sense_rate))\n",
    "        \n",
    "    return ensemble_tracker\n",
    "\n",
    "\n",
    "\n",
    "###RETURN ENSEMBLE STATS DATA###\n",
    "\n",
    "#set up models want to iterate through and get all combos of models for\n",
    "model_map = {0: unigram_pred_test_dense,1:bigram_pred_test_dense,2:trigram_pred_test_dense,\n",
    "          3:unigram_POS_pred_test_dense,4:bigram_POS_pred_test_dense,5:trigram_POS_pred_test_dense,\n",
    "             6:lstm_dense_preds_test_matched_index}\n",
    "\n",
    "#set up model names (used for prints)\n",
    "model_name_map = {0: 'unigram_pred_test_dense',1:'bigram_pred_test_dense',2:'trigram_pred_test_dense',\n",
    "          3:'unigram_POS_pred_test_dense',4:'bigram_POS_pred_test_dense',5:'trigram_POS_pred_test_dense',\n",
    "                 6: 'lstm_dense_preds_test_matched_index'}\n",
    "\n",
    "\n",
    "ensemble_data = grid_search_ensembles(model_map=model_map,model_name_map=model_name_map,\n",
    "                                         correct_labels=var_test_ids, max_samples=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS TO RETURN AND PRINT BEST ENSEMBLE INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP 5 MODELS:\n",
      "\n",
      "\n",
      "COMBINED MODELS: ['trigram_pred_test_dense', 'unigram_POS_pred_test_dense', 'bigram_POS_pred_test_dense']\n",
      "MODEL NUMBERS: (2, 3, 4)\n",
      "VOTING CLASSIFIER TYPE: SOFT\n",
      "NUM SAMPLES: 1850\n",
      "CORRECT PREDS:\t\t\t1156 /    62.5%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t821 /    44.4%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t912 /    49.3%\n",
      "\n",
      "COMBINED MODELS: ['unigram_POS_pred_test_dense', 'bigram_POS_pred_test_dense']\n",
      "MODEL NUMBERS: (3, 4)\n",
      "VOTING CLASSIFIER TYPE: SOFT\n",
      "NUM SAMPLES: 1850\n",
      "CORRECT PREDS:\t\t\t1152 /    62.3%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t921 /    49.8%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t1010 /    54.6%\n",
      "\n",
      "COMBINED MODELS: ['bigram_pred_test_dense', 'trigram_pred_test_dense', 'unigram_POS_pred_test_dense', 'bigram_POS_pred_test_dense']\n",
      "MODEL NUMBERS: (1, 2, 3, 4)\n",
      "VOTING CLASSIFIER TYPE: SOFT\n",
      "NUM SAMPLES: 1850\n",
      "CORRECT PREDS:\t\t\t1150 /    62.2%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t807 /    43.6%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t898 /    48.5%\n",
      "\n",
      "COMBINED MODELS: ['unigram_pred_test_dense', 'bigram_pred_test_dense', 'trigram_pred_test_dense', 'unigram_POS_pred_test_dense', 'bigram_POS_pred_test_dense']\n",
      "MODEL NUMBERS: (0, 1, 2, 3, 4)\n",
      "VOTING CLASSIFIER TYPE: SOFT\n",
      "NUM SAMPLES: 1850\n",
      "CORRECT PREDS:\t\t\t1149 /    62.1%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t806 /    43.5%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t897 /    48.5%\n",
      "\n",
      "COMBINED MODELS: ['bigram_pred_test_dense', 'unigram_POS_pred_test_dense', 'bigram_POS_pred_test_dense']\n",
      "MODEL NUMBERS: (1, 3, 4)\n",
      "VOTING CLASSIFIER TYPE: SOFT\n",
      "NUM SAMPLES: 1850\n",
      "CORRECT PREDS:\t\t\t1148 /    62.1%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t868 /    46.9%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t959 /    51.8%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def return_top_ensemble_info(ensemble_data, n=5) :\n",
    "    #RETURN LIST OF TOP ENSEMBLE INFO (use if need to reference model numbers, model names directly)\n",
    "    \n",
    "    #sort by number correct\n",
    "    ensemble_data.sort(key=lambda x: x[4], reverse=True)\n",
    "    \n",
    "    #get top n models\n",
    "    ensemble_data = ensemble_data[0:n]\n",
    "    \n",
    "    return ensemble_data\n",
    "\n",
    "\n",
    "def print_top_ensemble_info(ensemble_data, n=5) :\n",
    "\n",
    "    print \"TOP\", str(n),\"MODELS:\"\n",
    "    print \"\\n\"\n",
    "\n",
    "    #sort by number correct\n",
    "    ensemble_data.sort(key=lambda x: x[4], reverse=True)\n",
    "    \n",
    "    #get top n models\n",
    "    ensemble_data = ensemble_data[0:n]\n",
    "    \n",
    "    for item in ensemble_data[0:n]:\n",
    "        model_names_list = item[0]\n",
    "        model_nums = item[1]\n",
    "        voting_classifier = item[2]\n",
    "        num_samples = item[3]\n",
    "        correct_pred = item[4]\n",
    "        correct_pred_weight = item[5]\n",
    "        weight_correct_sense = item[6]\n",
    "        correct_pred_rate = item[7]\n",
    "        correct_pred_weight_rate = item[8]\n",
    "        weight_correct_sense_rate = item[9]\n",
    "\n",
    "\n",
    "        print \"COMBINED MODELS:\", model_names_list\n",
    "        print \"MODEL NUMBERS:\", model_nums\n",
    "        print \"VOTING CLASSIFIER TYPE:\", voting_classifier\n",
    "        print \"NUM SAMPLES:\", num_samples\n",
    "        print \"CORRECT PREDS:\\t\\t\\t\", correct_pred, \"/   \", '{:.1%}'.format(correct_pred_rate)\n",
    "        print \"CORRECT PRED WEIGHT ASSIGNED:\\t\", \"{:0.0f}\".format(correct_pred_weight), \"/   \", '{:.1%}'.format(correct_pred_weight_rate) \n",
    "        print \"TOTAL WEIGHT TO CORRECT SENSE:\\t\", \"{:0.0f}\".format(weight_correct_sense), \"/   \", '{:.1%}'.format(weight_correct_sense_rate)\n",
    "        print \n",
    "\n",
    "\n",
    "#use if want to reference model numbers, model names, scores directly\n",
    "top_ensemble_info = return_top_ensemble_info(ensemble_data=ensemble_data, n=5)\n",
    "\n",
    "print_top_ensemble_info(ensemble_data=ensemble_data, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS TO COMPARE MODEL ENSEMBLES WITH AND WITHOUT SINGLE MODEL\n",
    "Note: Only to be used when running on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH TARGET MODEL # 0 :\n",
      "MODEL: unigram_pred_test_dense\n",
      "NUM ENSEMBLES: 126 126\n",
      "ACCURACY WITH: 57.5%\n",
      "ACCURACY WITHOUT: 57.6%\n",
      "AVERAGE CONTRIBUTION: -0.0%\n",
      "\n",
      "WITH TARGET MODEL # 1 :\n",
      "MODEL: bigram_pred_test_dense\n",
      "NUM ENSEMBLES: 126 126\n",
      "ACCURACY WITH: 57.7%\n",
      "ACCURACY WITHOUT: 57.3%\n",
      "AVERAGE CONTRIBUTION: 0.4%\n",
      "\n",
      "WITH TARGET MODEL # 2 :\n",
      "MODEL: trigram_pred_test_dense\n",
      "NUM ENSEMBLES: 126 126\n",
      "ACCURACY WITH: 57.5%\n",
      "ACCURACY WITHOUT: 57.6%\n",
      "AVERAGE CONTRIBUTION: -0.1%\n",
      "\n",
      "WITH TARGET MODEL # 3 :\n",
      "MODEL: unigram_POS_pred_test_dense\n",
      "NUM ENSEMBLES: 126 126\n",
      "ACCURACY WITH: 58.8%\n",
      "ACCURACY WITHOUT: 56.2%\n",
      "AVERAGE CONTRIBUTION: 2.6%\n",
      "\n",
      "WITH TARGET MODEL # 4 :\n",
      "MODEL: bigram_POS_pred_test_dense\n",
      "NUM ENSEMBLES: 126 126\n",
      "ACCURACY WITH: 58.7%\n",
      "ACCURACY WITHOUT: 56.2%\n",
      "AVERAGE CONTRIBUTION: 2.5%\n",
      "\n",
      "WITH TARGET MODEL # 5 :\n",
      "MODEL: trigram_POS_pred_test_dense\n",
      "NUM ENSEMBLES: 126 126\n",
      "ACCURACY WITH: 58.1%\n",
      "ACCURACY WITHOUT: 56.9%\n",
      "AVERAGE CONTRIBUTION: 1.2%\n",
      "\n",
      "WITH TARGET MODEL # 6 :\n",
      "MODEL: lstm_dense_preds_test_matched_index\n",
      "NUM ENSEMBLES: 126 126\n",
      "ACCURACY WITH: 57.2%\n",
      "ACCURACY WITHOUT: 58.2%\n",
      "AVERAGE CONTRIBUTION: -1.1%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_ensemble_target_model_info(ensemble_data,var_test_ids, model_name_map, model_map, target_model) :\n",
    "    #sort by number correct\n",
    "    ensemble_data.sort(key=lambda x: x[4], reverse=True)\n",
    "    \n",
    "    #get target models\n",
    "    target_model_ensembles = []\n",
    "    target_model_ensembles_num = 0\n",
    "    target_model_ensembles_rate = 0\n",
    "    #get exact same ensembles, just without target\n",
    "    same_ensembles_without_target = []\n",
    "    same_ensembles_without_target_num = 0\n",
    "    same_ensembles_without_target_rate = 0\n",
    "    \n",
    "    #get models lists\n",
    "    for item in ensemble_data:         \n",
    "        model_nums = item[1]\n",
    "        #models with target model in and at least three models\n",
    "        if target_model in model_nums and len(model_nums) > 1:\n",
    "            target_model_ensembles.append(model_nums)\n",
    "    for models in target_model_ensembles :\n",
    "        #remove target\n",
    "        models_list = []\n",
    "        for model in models :\n",
    "            if model <> target_model :\n",
    "                models_list.append(model)\n",
    "        models_list = tuple(models_list)\n",
    "        same_ensembles_without_target.append(models_list)\n",
    "    \n",
    "    \n",
    "    for item in ensemble_data:\n",
    "        model_names_list = item[0]\n",
    "        model_nums = item[1]\n",
    "        voting_classifier = item[2]\n",
    "        num_samples = item[3]\n",
    "        correct_pred = item[4]\n",
    "        correct_pred_weight = item[5]\n",
    "        weight_correct_sense = item[6]\n",
    "        correct_pred_rate = item[7]\n",
    "        correct_pred_weight_rate = item[8]\n",
    "        weight_correct_sense_rate = item[9]\n",
    "        \n",
    "        \n",
    "        if model_nums in target_model_ensembles:\n",
    "            target_model_ensembles_num += 1\n",
    "            target_model_ensembles_rate += correct_pred_rate\n",
    "            \n",
    "        if model_nums in same_ensembles_without_target:\n",
    "            same_ensembles_without_target_num += 1\n",
    "            same_ensembles_without_target_rate += correct_pred_rate\n",
    "        \n",
    "        #get single model\n",
    "        if len(model_nums) == 2 and model_nums in target_model_ensembles:\n",
    "            #remove target\n",
    "            single_model = None\n",
    "            for model in model_nums :\n",
    "                if model <> target_model :\n",
    "                    single_model = model\n",
    "                    samples,correct,_,_ = return_model_stats(\n",
    "                      model_map[single_model], var_test_ids, max_samples = None)\n",
    "            same_ensembles_without_target_num +=1 \n",
    "            same_ensembles_without_target_rate += float(correct)/float(samples)\n",
    "    \n",
    "    #normalize\n",
    "    target_model_ensembles_rate = float(target_model_ensembles_rate)/target_model_ensembles_num\n",
    "    same_ensembles_without_target_rate = float(same_ensembles_without_target_rate)/same_ensembles_without_target_num\n",
    "    target_model_contribution = target_model_ensembles_rate - same_ensembles_without_target_rate\n",
    "        \n",
    "    print \"WITH TARGET MODEL #\", target_model, \":\"\n",
    "    print \"MODEL:\", model_name_map[target_model]\n",
    "    print \"NUM ENSEMBLES:\", target_model_ensembles_num, same_ensembles_without_target_num\n",
    "    print \"ACCURACY WITH:\", '{:.1%}'.format(target_model_ensembles_rate)\n",
    "    print \"ACCURACY WITHOUT:\", '{:.1%}'.format(same_ensembles_without_target_rate)    \n",
    "    print \"AVERAGE CONTRIBUTION:\", '{:.1%}'.format(target_model_contribution)\n",
    "    print \n",
    "        \n",
    "print_ensemble_target_model_info(ensemble_data=ensemble_data, var_test_ids=var_test_ids, model_map=model_map, model_name_map=model_name_map, target_model=0)\n",
    "print_ensemble_target_model_info(ensemble_data=ensemble_data, var_test_ids=var_test_ids,model_map=model_map,model_name_map=model_name_map,target_model=1)\n",
    "print_ensemble_target_model_info(ensemble_data=ensemble_data, var_test_ids=var_test_ids,model_map=model_map,model_name_map=model_name_map,target_model=2)\n",
    "print_ensemble_target_model_info(ensemble_data=ensemble_data, var_test_ids=var_test_ids,model_map=model_map,model_name_map=model_name_map,target_model=3)\n",
    "print_ensemble_target_model_info(ensemble_data=ensemble_data, var_test_ids=var_test_ids,model_map=model_map,model_name_map=model_name_map,target_model=4)\n",
    "print_ensemble_target_model_info(ensemble_data=ensemble_data, var_test_ids=var_test_ids,model_map=model_map,model_name_map=model_name_map,target_model=5)\n",
    "print_ensemble_target_model_info(ensemble_data=ensemble_data, var_test_ids=var_test_ids,model_map=model_map,model_name_map=model_name_map,target_model=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-456-2304cb7c12f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#INTENTIONALLY CREATE ERROR TO MAKE CODE STOP RUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ERROR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "#INTENTIONALLY CREATE ERROR TO MAKE CODE STOP RUNNING\n",
    "error = {}\n",
    "error.append('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENT STOCHASTIC GRADIENT DESCENT TO GET OPTIMAL WEIGHTS FOR AN ENSEMBLE\n",
    " - Note: Only built for soft classifier as results clearly show soft classifier is superior\n",
    " - Note: Only use to find optimal models in test set; not in Eval set. Must reuse optimal weights from test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### SHOULD I IMPLEMENT WEIGHTS RANDOMLY THEN AVERAGE THEM AFTERWARDS?\n",
    "\n",
    "def get_optimal_weights_soft(model_nums, model_map, model_name_map, correct_labels,weight_step=.01, \n",
    "                        epsilon=0.001, num_samples=None, num_epochs=5, batch_size=10,\n",
    "                             random_init=False, random_sampling=False) :\n",
    "    \n",
    "    \"\"\"IMPLEMENT STOCHASTIC GRADIENT DESCENT TO GET OPTIMAL WEIGHTS (at least locally optimal)\n",
    "    model_nums = integers corresponding to model index number\n",
    "    model_map = map containing model dense predictions by model index number\n",
    "    model_name_map = map containing model name by model index number\n",
    "    correct_labels = correct word sense IDs (must be ordered)\n",
    "    weight_step = within stochastic weightings, how much to increase target weight\n",
    "    epsilon = if target weight resulted in increase to # correct, increase output weights by epsilon\n",
    "    num_samples = Maximum number of samples (cuts off after)\n",
    "    num_epochs = number of times to iterate through full prediction set\n",
    "    batch_size = size of batch to run stochastic gradient descent on\n",
    "    random_init = initialize weights randomly (True/False); False defaults to even original weights list\n",
    "    random_sampling = sample batches (True/False); False defaults to iterate through indexes in order\n",
    "    \"\"\"\n",
    "    \n",
    "    n_models = len(model_nums)\n",
    "    \n",
    "    #if model weight is < weight_step (technically, (weight_step/remaining models)), we don't want to decrease it any more or it\n",
    "    #could go below 0 weight\n",
    "    models_under_lower_limit = 0\n",
    "    model_lower_limit = weight_step\n",
    "\n",
    "    model_upper_limit = 1-weight_step\n",
    "    \n",
    "    if num_samples == None :\n",
    "        num_samples = len(correct_labels)\n",
    "    else :\n",
    "        correct_labels = correct_labels[0:num_samples]\n",
    "    \n",
    "    if random_init is False :\n",
    "        #initialize weights evenly\n",
    "        weights_list = [1/float(n_models) for model in range(n_models)]\n",
    "    else :\n",
    "        #initialize randomly and normalize to 1\n",
    "        weights_list = np.random.random_sample(n_models)\n",
    "        weights_list /= weights_list.sum()\n",
    "        weights_list.tolist()\n",
    "    \n",
    "    \n",
    "    #get models, make sure IDs are in sorted order\n",
    "    model_list = [sorted(model_map[num].iteritems()) for num in model_nums]\n",
    "    model_names_list = [model_name_map[num] for num in model_nums]\n",
    "    \n",
    "    weights_list = list(weights_list)\n",
    "    \n",
    "    iteration_counter = 0\n",
    "    for iteration in range(num_epochs) :\n",
    "        \n",
    "        #set initial batch slices\n",
    "        min_slice = 0\n",
    "        max_slice = batch_size\n",
    "                \n",
    "        #get number of batches\n",
    "        if num_samples % batch_size == 0 :\n",
    "            num_batches = num_samples/batch_size\n",
    "        else :\n",
    "            num_batches = int(num_samples/batch_size) + 1\n",
    "        \n",
    "        for batch in range(num_batches) :\n",
    "\n",
    "            if random_sampling is True :\n",
    "                #override min_slice and max_slice to a randomly chosen sample\n",
    "                min_slice = np.random.randint(0, num_samples)\n",
    "                max_slice = min_slice+10\n",
    "            \n",
    "            #get act_index, model predictions in batch size of instance IDs            \n",
    "            ids_batch = correct_labels[min_slice:max_slice]\n",
    "\n",
    "            #get corresponding model list\n",
    "            model_list_batch = [dict(model[min_slice:max_slice]) for model in model_list]            \n",
    "\n",
    "\n",
    "            #GET NUMBER CORRECT ASSIGNMENTS IN BATCH\n",
    "            hard, soft = voting_classifier_dense_preds(pred_dense_model_list=model_list_batch, ids_array=ids_batch,\n",
    "                                 weights_list=weights_list, max_samples=None)\n",
    "            batch_correct = return_model_stats(soft, ids_array=ids_batch, max_samples=None)[1]\n",
    "            \n",
    "            \"\"\"PERFORM STOCHASTIC GRADIENT DESCENT; get new model weights\"\"\"\n",
    "            ### If moving up the model_weight of an individual model results in a greater batch_correct...\n",
    "            ##### then increase the model_weight (decrementing others)\n",
    "            \n",
    "            #get initial weights assigned to model ensemble\n",
    "            counter = 0\n",
    "            #for each model, perform stochastic gradient descent\n",
    "            for model_preds in model_list_batch :\n",
    "                #Modifies from weights style [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "                #to [0.201, 0.1998, 0.1998, 0.1998, 0.1998] ... then bumps up weight by epsilon (if improves)\n",
    "                \n",
    "                #reset starting point to weights_list. #MUST MAKE COPY OF LIST; cannot point to original\n",
    "                stochastic_weights_list = list(weights_list)\n",
    "                \n",
    "                #get reduction amount to apply to non-target weights\n",
    "                above_model_lower_limit = sum(w>=model_lower_limit for w in stochastic_weights_list)\n",
    "                if stochastic_weights_list[counter] >= model_lower_limit :\n",
    "                    above_model_lower_limit -= 1\n",
    "                    #exclude target weight from count\n",
    "\n",
    "                if above_model_lower_limit == 0 :\n",
    "                    #don't want to do any modifications if none to decrement... should be rate\n",
    "                    continue\n",
    "                reduction_amount_step = weight_step/float(above_model_lower_limit)\n",
    "                reduction_amount_epsilon = epsilon/float(above_model_lower_limit)\n",
    "                \n",
    "                #increment stochastic_weights_list by WEIGHTS_STEP\n",
    "                if stochastic_weights_list[counter] <= model_upper_limit :\n",
    "                    stochastic_weights_list[counter] += weight_step\n",
    "                else :\n",
    "                    #move to next model if beyond upper limit\n",
    "                    continue\n",
    "                    \n",
    "                #decrement non-target weights according to WEIGHTS_STEP\n",
    "                for w_index, weight in enumerate(stochastic_weights_list) :\n",
    "                    if w_index == counter :\n",
    "                        pass\n",
    "                    else :\n",
    "                        if stochastic_weights_list[w_index] >= model_lower_limit :\n",
    "                            stochastic_weights_list[w_index] = weight-reduction_amount_step\n",
    "\n",
    "                #normalize back to 1 (can sometimes end up 1.00000000001 due to rounding)\n",
    "                stochastic_weights_list = np.asarray(stochastic_weights_list)\n",
    "                stochastic_weights_list /= sum(stochastic_weights_list)\n",
    "                stochastic_weights_list.tolist()                \n",
    "                \n",
    "                #run through voting_classifier\n",
    "                hard, soft = voting_classifier_dense_preds(pred_dense_model_list=model_list_batch, ids_array=ids_batch,\n",
    "                                     weights_list=stochastic_weights_list, max_samples=None)\n",
    "                stochastic_batch_correct = return_model_stats(soft, ids_array=ids_batch, max_samples=None)[1]\n",
    "                                                \n",
    "                #update weight_list by EPSILON when stochastic gets better score than original batch score\n",
    "                if stochastic_batch_correct > batch_correct :\n",
    "                    weights_list[counter] += epsilon\n",
    "\n",
    "                    #decrement non-target weights\n",
    "                    for w_index, weight in enumerate(weights_list) :\n",
    "                        if w_index == counter :\n",
    "                            pass\n",
    "                        else :\n",
    "                            if weights_list[w_index] >= model_lower_limit :\n",
    "                                weights_list[w_index] = weight-reduction_amount_epsilon\n",
    "                    \n",
    "                    #normalize weights_list:\n",
    "                    weights_list=np.asarray(weights_list)\n",
    "                    weights_list /= weights_list.sum()\n",
    "                    weights_list.tolist()\n",
    "                                                                \n",
    "                counter += 1\n",
    "\n",
    "            #increment/move to next batch slice\n",
    "            min_slice += batch_size\n",
    "            max_slice += batch_size\n",
    "        \n",
    "        iteration_counter += 1\n",
    "        print \"ITERATION #:\", iteration_counter\n",
    "        print \"WEIGHTS:\", weights_list\n",
    "    \n",
    "    #normalize back to 1 (can sometimes end up 1.00000000001 due to rounding)\n",
    "    weights_list = np.asarray(weights_list)\n",
    "    weights_list /= sum(weights_list)\n",
    "    weights_list.tolist()\n",
    "    return weights_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RETURN OPTIMAL WEIGHTS (PER GRADIENT DESCENT)\n",
    " - Note: Only use to find optimal models in test set; not in Eval set. Must reuse optimal weights from test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL NUMBERS: (0, 3, 4)\n",
    "MODEL NUMBERS: (0, 2, 3, 4)\n",
    "MODEL NUMBERS: (3, 4)\n",
    "MODEL NUMBERS: (0, 2, 3, 4, 5)\n",
    "MODEL NUMBERS: (0, 2, 3, 5)\n",
    "all models - [0.02477381, 0.03094048,0.02444048,0.28677381,0.22635714,0.23502381,0.17169048]\n",
    "[0,2,3,4] = [0.205,0.23166667,0.225,0.33833333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #: 1\n",
      "WEIGHTS: [0.25, 0.25, 0.25, 0.25]\n",
      "ITERATION #: 2\n",
      "WEIGHTS: [0.25, 0.25, 0.25, 0.25]\n",
      "ITERATION #: 3\n",
      "WEIGHTS: [0.25, 0.25, 0.25, 0.25]\n"
     ]
    }
   ],
   "source": [
    "# model_nums=[1, 2, 3, 4, 5]\n",
    "model_nums=[0, 2, 3, 4]# - include all\n",
    "\n",
    "#BASELINE\n",
    "# optimal_weights = get_optimal_weights_soft(model_nums=model_nums, model_map=model_map, model_name_map=model_name_map, \n",
    "#                          correct_labels=var_test_ids, \n",
    "#                         weight_step=0.025, epsilon=0.01, num_samples=None, num_epochs=10, batch_size=10,\n",
    "#                                           random_init=False, random_sampling=False)\n",
    "\n",
    "\n",
    "\n",
    "#USE\n",
    "optimal_weights = get_optimal_weights_soft(model_nums=model_nums, model_map=model_map, model_name_map=model_name_map, \n",
    "                         correct_labels=var_test_ids, \n",
    "                        weight_step=0.025, epsilon=0.005, num_samples=None, num_epochs=3, batch_size=10,\n",
    "                                          random_init=False, random_sampling=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RETURN KEY MODEL STATS WITH OPTIMAL WEIGHTS INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def return_models_from_nums(model_nums, model_map) :\n",
    "    #get models, make sure IDs are in sorted order\n",
    "    model_list = [model_map[num] for num in model_nums]\n",
    "    return model_list\n",
    "\n",
    "def return_model_names_from_nums(model_nums, model_name_map) :\n",
    "    model_names_list = [model_name_map[num] for num in model_nums]\n",
    "    return model_names_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN KEY MODEL STATS WITH OPTIMAL WEIGHTS INPUT (FROM TEST) ON OPTIMAL ENSEMBLE (FROM TEST) AND ALL-MODEL ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOFT VOTING CLASSIFIER - VARIABLE INPUT:\n",
      "['unigram_pred_test_dense', 'trigram_pred_test_dense', 'unigram_POS_pred_test_dense', 'bigram_POS_pred_test_dense']\n",
      "NUM SAMPLES:\t\t\t2532\n",
      "CORRECT PREDS:\t\t\t2 /    0.1%\n",
      "CORRECT PRED WEIGHT ASSIGNED:\t0.0 /    0.0%\n",
      "TOTAL WEIGHT TO CORRECT SENSE:\t0.0 /    0.0%\n"
     ]
    }
   ],
   "source": [
    "## Soft voting classifier - VARIABLE INPUT\n",
    "model_list = return_models_from_nums(model_nums=model_nums, model_map=model_map)\n",
    "model_names_list = return_model_names_from_nums(model_nums=model_nums, model_name_map=model_name_map)\n",
    "\n",
    "hard, soft = voting_classifier_dense_preds(pred_dense_model_list=model_list, ids_array=var_test_ids,\n",
    "                                     weights_list=optimal_weights, max_samples=None)\n",
    "print \"SOFT VOTING CLASSIFIER - VARIABLE INPUT:\"\n",
    "print model_names_list\n",
    "print_model_stats(soft, var_test_ids, max_samples=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_nums=[0, 2, 3, 4]# - include all\n",
    "optimal_weights = [0.205,0.23166667,0.225,0.33833333]\n",
    "\n",
    "model_list = return_models_from_nums(model_nums=model_nums, model_map=model_map)\n",
    "model_names_list = return_model_names_from_nums(model_nums=model_nums, model_name_map=model_name_map)\n",
    "\n",
    "hard, soft = voting_classifier_dense_preds(pred_dense_model_list=model_list, ids_array=var_test_ids,\n",
    "                                     weights_list=optimal_weights, max_samples=None)\n",
    "print \"SOFT VOTING CLASSIFIER STATS (OPTIMAL ENSEMBLE):\"\n",
    "print model_names_list\n",
    "print_model_stats(soft, var_test_ids, max_samples=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_nums=[0, 1, 2, 3, 4, 5, 6]# - include all\n",
    "optimal_weights = [0.02477381, 0.03094048,0.02444048,0.28677381,0.22635714,0.23502381,0.17169048]\n",
    "\n",
    "model_list = return_models_from_nums(model_nums=model_nums, model_map=model_map)\n",
    "model_names_list = return_model_names_from_nums(model_nums=model_nums, model_name_map=model_name_map)\n",
    "\n",
    "hard, soft = voting_classifier_dense_preds(pred_dense_model_list=model_list, ids_array=var_test_ids,\n",
    "                                     weights_list=optimal_weights, max_samples=None)\n",
    "print \"SOFT VOTING CLASSIFIER STATS (ALL ENSEMBLES):\"\n",
    "print model_names_list\n",
    "print_model_stats(soft, var_test_ids, max_samples=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTION TO SEE ALL PREDICTED INDEXES BY MODEL OR MODEL ENSEMBLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def return_id_preds(model_nums, model_map, weights_list=None) :\n",
    "    \n",
    "    num_models = len(model_nums)\n",
    "    \n",
    "    if weights_list is None:\n",
    "        weights_list = [1/float(num_models) for m in model_map]\n",
    "    \n",
    "    pred_dense_model_list = [model_map[num] for num in model_nums]\n",
    "    \n",
    "    max_indexes = []\n",
    "    \n",
    "    combined_preds = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    \n",
    "    model_counter = 0\n",
    "    #first, iterate through individual models, add preds by index to dict\n",
    "    for model_preds in pred_dense_model_list :\n",
    "        #get key info from each predictive model output, append to a default_dict to be referenced later\n",
    "        \n",
    "        counter = 0\n",
    "        #run as soft classifier (weighted)\n",
    "        for (instance_id, preds) in sorted(model_preds.iteritems()) :            \n",
    "            \n",
    "            if preds is None :\n",
    "                #handle None from LSTM\n",
    "                combined_preds[counter][0] += 0\n",
    "                counter += 1\n",
    "                continue\n",
    "            \n",
    "            else :\n",
    "                for ws_index, prob in preds.iteritems() :\n",
    "                    combined_preds[counter][ws_index] += prob*weights_list[model_counter]\n",
    "                counter += 1\n",
    "        model_counter += 1\n",
    "                    \n",
    "    #then, iterate through dict created and append max to max_index\n",
    "    for (ws_index, preds) in sorted(combined_preds.iteritems()) :            \n",
    "        if preds is None :\n",
    "            #handle None from LSTM\n",
    "            max_index = None\n",
    "        else :\n",
    "            max_index = max(sorted(preds.iteritems()), key=operator.itemgetter(1))[0]\n",
    "        max_indexes.append(max_index)\n",
    "    max_indexes = np.asarray(max_indexes)\n",
    "    \n",
    "    return max_indexes, model_nums\n",
    "    \n",
    "    return max_indexes, model_nums\n",
    "model_list = [unigram_pred_test_dense,unigram_POS_pred_test_dense]\n",
    "print len(return_id_preds(model_nums=[1,2,3], model_map=model_map, weights_list=None)[0])\n",
    "print len(return_id_preds(model_nums=[6], model_map=model_map, weights_list=None)[0])\n",
    "print len(return_id_preds(model_nums=[0], model_map=model_map, weights_list=None)[0])\n",
    "print len(lstm_dense_preds_test_matched_index.keys())\n",
    "print len(lstm_dense_preds_test_matched_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Similarity of predictions between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.794054054054054, [0, 1, 2, 3, 4, 5, 6], [0])\n",
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.7967567567567567, [0, 1, 2, 3, 4, 5, 6], [1])\n",
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.7902702702702703, [0, 1, 2, 3, 4, 5, 6], [2])\n",
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.865945945945946, [0, 1, 2, 3, 4, 5, 6], [3])\n",
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.885945945945946, [0, 1, 2, 3, 4, 5, 6], [4])\n",
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.8708108108108108, [0, 1, 2, 3, 4, 5, 6], [5])\n",
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.4016216216216216, [0, 1, 2, 3, 4, 5, 6], [6])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#get the rate of same predictions between two models / model ensembles\n",
    "def compare_preds(return_ids_1, return_ids_2, model_name_map) :\n",
    "    ids_1 = return_ids_1[0]\n",
    "    ids_2 = return_ids_2[0]\n",
    "    \n",
    "    model_nums_1 = return_ids_1[1]\n",
    "    model_nums_2 = return_ids_2[1]\n",
    "    array_length = len(ids_1)\n",
    "    same_preds = np.sum(ids_1 == ids_2)\n",
    "    same_preds_rate = float(same_preds)/float(array_length)\n",
    "    \n",
    "    return same_preds_rate, model_nums_1, model_nums_2\n",
    "  \n",
    "#print output by model combination\n",
    "# for combo in itertools.combinations(model_map.keys(), 2):\n",
    "#     model_1 = [combo[0]]\n",
    "#     model_2 = [combo[1]]\n",
    "#     print compare_preds(return_id_preds(model_nums=model_1, model_map=model_map, weights_list=None), \n",
    "#               return_id_preds(model_nums=model_2, model_map=model_map, weights_list=None), \n",
    "#               model_name_map=model_name_map)\n",
    "\n",
    "  \n",
    "# #Compare Single Model example\n",
    "# print \"\\nMFS-LSTM:\", compare_preds(return_id_preds(model_nums=[0], model_map=model_map, weights_list=None), \n",
    "#               return_id_preds(model_nums=[6], model_map=model_map, weights_list=None), \n",
    "#               model_name_map=model_name_map), len(var_test_ids)\n",
    "  \n",
    "#ensembles\n",
    "# print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,2,3,4], model_map=model_map, weights_list=[0.205,0.23166667,0.225,0.33833333]), \n",
    "#               return_id_preds(model_nums=[0], model_map=model_map, weights_list=None), \n",
    "#               model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,1,2,3,4,5,6], model_map=model_map, weights_list=[0.02477381, 0.03094048,0.02444048,0.28677381,0.22635714,0.23502381,0.17169048]), \n",
    "              return_id_preds(model_nums=[0], model_map=model_map, weights_list=None), \n",
    "              model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,1,2,3,4,5,6], model_map=model_map, weights_list=[0.02477381, 0.03094048,0.02444048,0.28677381,0.22635714,0.23502381,0.17169048]), \n",
    "              return_id_preds(model_nums=[1], model_map=model_map, weights_list=None), \n",
    "              model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,1,2,3,4,5,6], model_map=model_map, weights_list=[0.02477381, 0.03094048,0.02444048,0.28677381,0.22635714,0.23502381,0.17169048]), \n",
    "              return_id_preds(model_nums=[2], model_map=model_map, weights_list=None), \n",
    "              model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,1,2,3,4,5,6], model_map=model_map, weights_list=[0.02477381, 0.03094048,0.02444048,0.28677381,0.22635714,0.23502381,0.17169048]), \n",
    "              return_id_preds(model_nums=[3], model_map=model_map, weights_list=None), \n",
    "              model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,1,2,3,4,5,6], model_map=model_map, weights_list=[0.02477381, 0.03094048,0.02444048,0.28677381,0.22635714,0.23502381,0.17169048]), \n",
    "              return_id_preds(model_nums=[4], model_map=model_map, weights_list=None), \n",
    "              model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,1,2,3,4,5,6], model_map=model_map, weights_list=[0.02477381, 0.03094048,0.02444048,0.28677381,0.22635714,0.23502381,0.17169048]), \n",
    "              return_id_preds(model_nums=[5], model_map=model_map, weights_list=None), \n",
    "              model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,1,2,3,4,5,6], model_map=model_map, weights_list=[0.02477381, 0.03094048,0.02444048,0.28677381,0.22635714,0.23502381,0.17169048]), \n",
    "              return_id_preds(model_nums=[6], model_map=model_map, weights_list=None), \n",
    "              model_name_map=model_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.8281081081081081, [0, 2, 3, 4], [0])\n",
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.8281081081081081, [0, 2, 3, 4], [1])\n",
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.8394594594594594, [0, 2, 3, 4], [2])\n",
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.907027027027027, [0, 2, 3, 4], [3])\n",
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.9340540540540541, [0, 2, 3, 4], [4])\n",
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.8313513513513513, [0, 2, 3, 4], [5])\n",
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.36162162162162165, [0, 2, 3, 4], [6])\n",
      "\n",
      "MFS-OPTIMAL ENSEMBLE: (0.9091891891891892, [0, 2, 3, 4], [0, 1, 2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#get the rate of same predictions between two models / model ensembles\n",
    "def compare_preds(return_ids_1, return_ids_2, model_name_map) :\n",
    "    ids_1 = return_ids_1[0]\n",
    "    ids_2 = return_ids_2[0]\n",
    "    \n",
    "    model_nums_1 = return_ids_1[1]\n",
    "    model_nums_2 = return_ids_2[1]\n",
    "    array_length = len(ids_1)\n",
    "    same_preds = np.sum(ids_1 == ids_2)\n",
    "    same_preds_rate = float(same_preds)/float(array_length)\n",
    "    \n",
    "    return same_preds_rate, model_nums_1, model_nums_2\n",
    "  \n",
    "#print output by model combination\n",
    "# for combo in itertools.combinations(model_map.keys(), 2):\n",
    "#     model_1 = [combo[0]]\n",
    "#     model_2 = [combo[1]]\n",
    "#     print compare_preds(return_id_preds(model_nums=model_1, model_map=model_map, weights_list=None), \n",
    "#               return_id_preds(model_nums=model_2, model_map=model_map, weights_list=None), \n",
    "#               model_name_map=model_name_map)\n",
    "\n",
    "  \n",
    "# #Compare Single Model example\n",
    "# print \"\\nMFS-LSTM:\", compare_preds(return_id_preds(model_nums=[0], model_map=model_map, weights_list=None), \n",
    "#               return_id_preds(model_nums=[6], model_map=model_map, weights_list=None), \n",
    "#               model_name_map=model_name_map), len(var_test_ids)\n",
    "  \n",
    "#ensembles\n",
    "# print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,2,3,4], model_map=model_map, weights_list=[0.205,0.23166667,0.225,0.33833333]), \n",
    "#               return_id_preds(model_nums=[0], model_map=model_map, weights_list=None), \n",
    "#               model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,2,3,4], model_map=model_map, weights_list=[0.205,0.23166667,0.225,0.33833333]), \n",
    "              return_id_preds(model_nums=[0], model_map=model_map, weights_list=None), \n",
    "              model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,2,3,4], model_map=model_map, weights_list=[0.205,0.23166667,0.225,0.33833333]), \n",
    "              return_id_preds(model_nums=[1], model_map=model_map, weights_list=None), \n",
    "              model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,2,3,4], model_map=model_map, weights_list=[0.205,0.23166667,0.225,0.33833333]), \n",
    "              return_id_preds(model_nums=[2], model_map=model_map, weights_list=None), \n",
    "              model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,2,3,4], model_map=model_map, weights_list=[0.205,0.23166667,0.225,0.33833333]), \n",
    "              return_id_preds(model_nums=[3], model_map=model_map, weights_list=None), \n",
    "              model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,2,3,4], model_map=model_map, weights_list=[0.205,0.23166667,0.225,0.33833333]), \n",
    "              return_id_preds(model_nums=[4], model_map=model_map, weights_list=None), \n",
    "              model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,2,3,4], model_map=model_map, weights_list=[0.205,0.23166667,0.225,0.33833333]), \n",
    "              return_id_preds(model_nums=[5], model_map=model_map, weights_list=None), \n",
    "              model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,2,3,4], model_map=model_map, weights_list=[0.205,0.23166667,0.225,0.33833333]), \n",
    "              return_id_preds(model_nums=[6], model_map=model_map, weights_list=None), \n",
    "              model_name_map=model_name_map)\n",
    "\n",
    "print \"\\nMFS-OPTIMAL ENSEMBLE:\", compare_preds(return_id_preds(model_nums=[0,2,3,4], model_map=model_map, weights_list=[0.205,0.23166667,0.225,0.33833333]), \n",
    "              return_id_preds(model_nums=[0,1,2,3,4,5,6], model_map=model_map, weights_list=[0.02477381, 0.03094048,0.02444048,0.28677381,0.22635714,0.23502381,0.17169048]), \n",
    "              model_name_map=model_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.205,0.23166667,0.225,0.33833333]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX & PRIOR CODE FOR REFERENCE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE FUNCTION TO CONVERT EXISTING DENSE (DICT-BASED) MATRIX TO A FULL SPARSE MATRIX BY *WORD SENSE INDEX NUMBER*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### CREATE A WAY TO CONVERT A DENSE DICTIONARY-BASED INDEX INTO A CONSISTENT SPARSE MATRIX\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def dense_to_sparse(probas, vocab_length) :\n",
    "    #convert dense representation of matrix into a sparse matrix by index number    \n",
    "    ##NOTE: Requires word sense INDEXES, not word senses\n",
    "    sparse_matrix = np.zeros(vocab_length)\n",
    "    for index, prob in probas.iteritems():\n",
    "        sparse_matrix[index] = prob\n",
    "    return sparse_matrix\n",
    "\n",
    "probas_tester_1_sparse = dense_to_sparse(probas_tester_1, len(ws_index))\n",
    "print \"FULL MATRIX:\", probas_tester_1_sparse\n",
    "print \"SINGLE PROBA INSTANCE:\", probas_tester_1_sparse[5442]\n",
    "print \"SUM OF TOTAL PROBA:\", sum(probas_tester_1_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace = go.Heatmap(z=[[1, 20, 30],\n",
    "                      [20, 1, 60],\n",
    "                      [30, 60, 1]])\n",
    "data=[trace]\n",
    "py.iplot(data, filename='basic-heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KEYS APPENDIX:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE CONVERT TO CONSISTENT SPARSE LIST(ARRAYS), *IN CONSISTENT SORTED ORDER*\n",
    " - NOTE: May not be needed since using voting classifier above to make ensembles instead of sparse arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print len(unigram_pred_train_dense.keys())\n",
    "\n",
    "\"\"\"NEED TO MAKE ITERATIVE VERSION ACROSS MODELS THAT PICKS UP WHERE LEFT OFF AFTER\"\"\"\n",
    "\n",
    "def dense_to_sparse_probas(dense_dict, vocab_length) :\n",
    "    #convert dense dictionary to sparse matrix by index number\n",
    "    print vocab_length\n",
    "    sparse_probas = []\n",
    "    for instance_id, probas in sorted(dense_dict.iteritems()):\n",
    "        sparse_probas.append(dense_to_sparse(probas, vocab_length))\n",
    "    return sparse_probas\n",
    "\n",
    "### WILL NEED TO USE STOCHASTIC GRADIENT REGRESSOR AND USE partial_fit() BY FEEDING IN BATCHES\n",
    "#http://scikit-learn.org/dev/auto_examples/applications/plot_out_of_core_classification.html#example-applications-plot-out-of-core-classification-py\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
    "\n",
    "\n",
    "#NOTE: USING TRAIN MAKES COMPUTER RUN OUT OF MEMORY\n",
    "# unigram_pred_train_sparse = dense_to_sparse_probas(dense_dict=unigram_pred_train_dense, vocab_length=len(ws_index))\n",
    "\n",
    "#NOTE: USING TEST ALSO MAKES COMPUTER RUN OUT OF MEMORY\n",
    "# unigram_pred_test_sparse = dense_to_sparse_probas(dense_dict=unigram_pred_test_dense, vocab_length=len(ws_index))\n",
    "# bigram_pred_test_sparse = dense_to_sparse_probas(dense_dict=bigram_pred_test_dense, vocab_length=len(ws_index))\n",
    "# trigram_pred_test_sparse = dense_to_sparse_probas(dense_dict=trigram_pred_test_dense, vocab_length=len(ws_index))\n",
    "# unigram_POS_pred_test_sparse = dense_to_sparse_probas(dense_dict=unigram_POS_pred_test_dense, vocab_length=len(ws_index))\n",
    "# bigram_POS_pred_test_sparse = dense_to_sparse_probas(dense_dict=bigram_POS_pred_test_dense, vocab_length=len(ws_index))\n",
    "# trigram_POS_pred_test_sparse = dense_to_sparse_probas(dense_dict=trigram_POS_pred_test_dense, vocab_length=len(ws_index))\n",
    "\n",
    "# print \"UNIGRAM PRED:\", unigram_pred_test_sparse\n",
    "# print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "n_samples, n_features = 10, 5\n",
    "np.random.seed(0)\n",
    "y = np.random.randn(n_samples)\n",
    "print y\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "print X\n",
    "clf = linear_model.SGDRegressor()\n",
    "fit = clf.fit(X, y)\n",
    "print fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "n_samples, n_features = 10, 5\n",
    "np.random.seed(0)\n",
    "y = [1, 2, 3, 4]\n",
    "print y\n",
    "X = [[1,2],\n",
    "     [2,3],\n",
    "     [3,4],\n",
    "     [4,5]\n",
    "    ]\n",
    "print X\n",
    "clf = linear_model.SGDRegressor()\n",
    "fit = clf.fit(X, y)\n",
    "print fit\n",
    "clf.predict([2.5, 3.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "n_samples, n_features = 10, 5\n",
    "np.random.seed(0)\n",
    "y = [1, 2, 3]\n",
    "print y\n",
    "X = [[[1,2],[1,2]],\n",
    "     [[1,2],[1,2]],\n",
    "     [[1,2],[1,2]]\n",
    "    ]\n",
    "print X\n",
    "clf = linear_model.SGDRegressor()\n",
    "fit = clf.fit(X, y)\n",
    "print fit\n",
    "clf.predict([2.5, 3.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"MODIFIED IN v4\"\n",
    "from collections import defaultdict\n",
    "\n",
    "def sense_counter(keys, split_by_POS=\"N\") :\n",
    "    \"\"\"RETURNS:\n",
    "       sense_counts: a defaultdict of counts of sense assignments per word in keys\n",
    "       \n",
    "       NOTE: When 2+ sense assignments exist for an instance, \n",
    "       returns just the first assigned instance\"\"\"\n",
    "    \n",
    "    sense_counts = defaultdict(dict)\n",
    "    \n",
    "    for key in keys:\n",
    "        key=key.strip()\n",
    "        #get ID, lemma, and WordNet key \n",
    "        try :\n",
    "            ID, lemma_WN_key = key.split(\" \")\n",
    "            lemma, WN_key = lemma_WN_key.split(\"%\")\n",
    "\n",
    "            if split_by_POS == \"Y\" :  #concatenate with POS number\n",
    "                lemma = lemma+WN_key[0]\n",
    "\n",
    "            \n",
    "        #note: many instances of double assignments\n",
    "        except :\n",
    "            #TAKES ONLY INITIAL INSTANCE OF WORDNET SENSE ASSIGNMENT WHEN THERE ARE 2+\n",
    "            ID, lemma_WN_key, extras = key.split(\" \",2)\n",
    "            lemma, WN_key = lemma_WN_key.split(\"%\")\n",
    "\n",
    "            if split_by_POS == \"Y\" :  #concatenate with POS number\n",
    "                lemma = lemma+WN_key[0]\n",
    "            \n",
    "        #add to dict new count\n",
    "        if lemma in sense_counts.keys() :\n",
    "            if WN_key in sense_counts[lemma].keys() :\n",
    "                sense_counts[lemma][WN_key] += 1\n",
    "            else :\n",
    "                sense_counts[lemma][WN_key] = 1\n",
    "        else :\n",
    "            sense_counts[lemma][WN_key] = 1\n",
    "\n",
    "    #return the original list of keys, reduced\n",
    "    return sense_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"EDITED IN V4\"\n",
    "print \"EXAMPLE COUNT OUTPUT FOR 'work':\"\n",
    "semcor_sense_counts = sense_counter(semcor_keys,split_by_POS=\"N\")\n",
    "semeval2007_sense_counts = sense_counter(semeval2007_keys,split_by_POS=\"N\")\n",
    "\n",
    "print semcor_sense_counts['work']\n",
    "print semcor_sense_counts['work1']\n",
    "print semeval2007_sense_counts['work']\n",
    "print semeval2007_sense_counts['work1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(semcor_keys)\n",
    "print len(semcor_sense_counts)\n",
    "print len(semeval2007_keys)\n",
    "print len(semeval2007_sense_counts)\n",
    "# print semcor_sense_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"MODIFIED IN v4\"\n",
    "import operator\n",
    "\n",
    "def get_MFSenses(sense_counts) :\n",
    "    \"\"\"Get a dictionary of most frequent senses per word in the data set\n",
    "    \n",
    "    RETURNS:\n",
    "    max_sense_dict: a dict of each word and corresponding Most Frequent Sense\n",
    "    \"\"\"\n",
    "    \n",
    "    MFS_dict = {}\n",
    "    for word, word_senses in sense_counts.iteritems() :\n",
    "        #each word_senses comes in format {'1:04:00::': 86, '2:35:02::': 20, '2:41:03::': 32, '1:04:01::': 27, '2:41:02::': 70, '1:09:00::': 11, '2:41:04::': 4, '1:06:00::': 76, '2:36:01::': 3, '2:41:05::': 3, '2:36:00::': 9, '2:38:02::': 2, '2:41:01::': 2, '2:36:09::': 2, '2:41:00::': 62, '1:06:01::': 3, '2:38:00::': 5, '2:29:00::': 9, '1:19:00::': 5}\n",
    "        try:\n",
    "            #get maximum sense\n",
    "            max_sense = max(word_senses.iteritems(), key=operator.itemgetter(1))[0]\n",
    "            #add maximum sense into dict\n",
    "            MFS_dict[word] = max_sense\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return MFS_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"EXAMPLE MFS:\"\n",
    "\n",
    "semcor_MFS = get_MFSenses(semcor_sense_counts)\n",
    "semeval2007_MFS = get_MFSenses(semeval2007_sense_counts)\n",
    "\n",
    "print semcor_MFS['work']\n",
    "# print semeval2007_MFS['work']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"MODIFIED IN V3\"\"\"\n",
    "\n",
    "#GET RANDOM ASSIGNMENT AND MOST FREQUENT COUNTS\n",
    "\n",
    "def get_ambiguity(sense_counts):\n",
    "    \"\"\"Get ambiguity and random baseline figures.\n",
    "    \n",
    "    RETURNS:\n",
    "    ambiguity: average number of senses per word\n",
    "    random_baseline: % correct of eval set when randomly guessing sense (based on training data)\n",
    "    \"\"\"\n",
    "    lemma_count = 0\n",
    "    total_count = 0\n",
    "    total_ambiguity = 0\n",
    "\n",
    "    #made up of lemma:{senses} where senses have counts\n",
    "    #single senses form: {'1:04:00::': 86, '2:35:02::': 20, '2:41:03::': 32, '1:04:01::': 27, '2:41:02::': 70, '1:09:00::': 11, '2:41:04::': 4, '1:06:00::': 76, '2:36:01::': 3, '2:41:05::': 3, '2:36:00::': 9, '2:38:02::': 2, '2:41:01::': 2, '2:36:09::': 2, '2:41:00::': 62, '1:06:01::': 3, '2:38:00::': 5, '2:29:00::': 9, '1:19:00::': 5}\n",
    "    #effectively gets to going through the data set, totaling the number of senses per lemma encountered, adding (weighting takes care of)\n",
    "    for lemma, senses in sense_counts.iteritems():\n",
    "        lemma_count += 1\n",
    "        lemma_ambiguity = len(senses.values())\n",
    "        lemma_weighting = sum(senses.values())\n",
    "        total_ambiguity += lemma_ambiguity*lemma_weighting\n",
    "        total_count += lemma_weighting\n",
    "\n",
    "    ambiguity = float(total_ambiguity) / float(total_count)\n",
    "    random_baseline =  1/ambiguity\n",
    "\n",
    "    return ambiguity, random_baseline\n",
    "\n",
    "print \"AMBIGUITY, RANDOM GUESSING WITHIN SET:\"\n",
    "print get_ambiguity(semcor_sense_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(var_test_ids==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1-float(np.sum(var_test_ids==0))/var_test_ids.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"MODIFIED IN V3\"\"\"\n",
    "\n",
    "def get_baselines(MFSenses_train, sense_counts_train, sense_counts_eval):\n",
    "    \"\"\"Get keys baseline figures.\n",
    "    \n",
    "    RETURNS:\n",
    "    chooseMFS_baseline: % correct of eval set when picking most common sense (based on training data)\n",
    "    ceiling_senses_baseline: % of word senses for lemmas in eval set also in train set (max possible to get right)\n",
    "    ceiling_lemmas_baseline: % of lemmas in eval set also in train set\n",
    "    \"\"\"\n",
    "    total_count = 0\n",
    "    MFS_correct_count = 0\n",
    "    ceiling_count = 0\n",
    "        \n",
    "    #made up of lemma:{senses} where senses have counts\n",
    "    for lemma, senses in sense_counts_eval.iteritems():\n",
    "        #lemma format: 'work'\n",
    "        #senses format: '{'1:04:00::': 86, '2:35:02::': 20, '2:41:03::': 32, '1:04:01::': 27, '2:41:02::': 70, '1:09:00::': 11, '2:41:04::': 4, '1:06:00::': 76, '2:36:01::': 3, '2:41:05::': 3, '2:36:00::': 9, '2:38:02::': 2, '2:41:01::': 2, '2:36:09::': 2, '2:41:00::': 62, '1:06:01::': 3, '2:38:00::': 5, '2:29:00::': 9, '1:19:00::': 5}'\n",
    "        \n",
    "        for key, count in senses.iteritems():\n",
    "            \n",
    "            #if most common key is the current key, add to correct count\n",
    "            try :\n",
    "                if MFSenses_train[lemma] == key :\n",
    "                    MFS_correct_count += count\n",
    "            except :\n",
    "                pass            \n",
    "            \n",
    "            if key in sense_counts_train[lemma].keys() :\n",
    "                ceiling_count += count\n",
    "            \n",
    "        total_count += sum(senses.values())        \n",
    "        \n",
    "    #Calculate baseline metrics\n",
    "    chooseMFS_baseline = float(MFS_correct_count) / float(total_count)   \n",
    "    \n",
    "    ceiling_senses_baseline = float(ceiling_count) / float(total_count)    \n",
    "    ceiling_lemmas_baseline = len(set(sense_counts_eval.keys())&set(MFSenses_train.keys()))/float(len(set(sense_counts_eval.keys())))\n",
    "    \n",
    "    return chooseMFS_baseline, ceiling_senses_baseline, ceiling_lemmas_baseline\n",
    "\n",
    "print get_baselines(semcor_MFS,semcor_sense_counts, semeval2007_sense_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"ADDED IN V4\"\"\"\n",
    "###SEMCOR BASIS (TAKES A FEW MINUTES)\n",
    "###GET AND PRINT BASELINE METRICS\n",
    "names = ['semcor', 'semeval2007','semeval2013','semeval2015','senseval2','senseval3','semcor_train', 'semcor_test']\n",
    "train_data = semcor_keys\n",
    "eval_data = [semcor_keys, semeval2007_keys, semeval2013_keys, semeval2015_keys, senseval2_keys, senseval3_keys,\n",
    "             semcortrain_keys,semcortest_keys] \n",
    "\n",
    "#get training counts, most frequent terms\n",
    "sense_counts_train = sense_counter(train_data)\n",
    "MFSenses_train = get_MFSenses(sense_counts_train)\n",
    "\n",
    "#get eval metrics by comparing against train data\n",
    "for e, n in zip(eval_data, names) :\n",
    "    sense_counts_eval = sense_counter(e)\n",
    "    MFSenses_eval = get_MFSenses(sense_counts_eval)\n",
    "    MFS_baseline, ceiling_senses_baseline, ceiling_lemmas_baseline = get_baselines(\n",
    "        MFSenses_train,sense_counts_train,sense_counts_eval)\n",
    "    ambiguity = get_ambiguity(sense_counts_eval)\n",
    "    print \"FOR:\", n\n",
    "    print \"AMBIGUITY:\", ambiguity\n",
    "    print \"MFS BASELINE:\", MFS_baseline\n",
    "    print \"CEILING BASELINE (SENSE BASIS):\", ceiling_senses_baseline\n",
    "    print \"CEILING BASELINE (LEMMA BASIS):\", ceiling_lemmas_baseline\n",
    "    print \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"ADDED IN V5\"\"\"\n",
    "###SEMCOR+OMSTI BASIS (TAKES A WHILE TO RUN - COMMENT OUT IF NOT NEEDED)\n",
    "###GET AND PRINT BASELINE METRICS\n",
    "names = ['semcor_omsti', 'semeval2007','semeval2013','semeval2015','senseval2','senseval3']\n",
    "train_data = semcor_omsti_keys\n",
    "eval_data = [semcor_omsti_keys, semeval2007_keys, semeval2013_keys, semeval2015_keys, senseval2_keys, senseval3_keys] \n",
    "\n",
    "#get training counts, most frequent terms\n",
    "sense_counts_train = sense_counter(train_data)\n",
    "MFSenses_train = get_MFSenses(sense_counts_train)\n",
    "\n",
    "#get eval metrics by comparing against train data\n",
    "for e, n in zip(eval_data, names) :\n",
    "    sense_counts_eval = sense_counter(e)\n",
    "    MFSenses_eval = get_MFSenses(sense_counts_eval)\n",
    "    MFS_baseline, ceiling_senses_baseline, ceiling_lemmas_baseline = get_baselines(\n",
    "        MFSenses_train,sense_counts_train,sense_counts_eval)\n",
    "    ambiguity = get_ambiguity(sense_counts_eval)\n",
    "    print \"FOR:\", n\n",
    "    print \"AMBIGUITY:\", ambiguity\n",
    "    print \"MFS BASELINE:\", MFS_baseline\n",
    "    print \"CEILING BASELINE (SENSE BASIS):\", ceiling_senses_baseline\n",
    "    print \"CEILING BASELINE (LEMMA BASIS):\", ceiling_lemmas_baseline\n",
    "    print \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROOT APPENDIX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data=('semcor_root','semeval2007_root','semeval2013_root','semeval2015_root','senseval2_root','senseval3_root')\n",
    "data=(semcor_root,semcortrain_root,semcortest_root,semeval2007_root,semeval2013_root,semeval2015_root,senseval2_root,senseval3_root,)\n",
    "for h in data:\n",
    "    sents = 0\n",
    "    docs = 0\n",
    "    tokens = 0\n",
    "    anno = 0\n",
    "    for i in h.getchildren():\n",
    "        docs += 1\n",
    "        for j in i.getchildren():\n",
    "            sents += 1\n",
    "            tokens += len(j.getchildren())\n",
    "            for k in j.getchildren():\n",
    "                if k.tag == 'instance':\n",
    "                    anno+=1\n",
    "    print '-'*25\n",
    "    print h.attrib['source']\n",
    "    print '-'*25\n",
    "    print \"Documents: \",docs\n",
    "    print \"Sentences: \",sents\n",
    "    print \"Tokens: \",tokens\n",
    "    print \"Annotations: \",anno\n",
    "    print '-'*25\n",
    "    print '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root.getchildren()[0].getchildren()[0].getchildren()[0].tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"NOTE: STILL NOT PICKING UP WORD AT THE END OF THE INSTANCE! NEED TO DO SO, RIGHT? OTHERWISE JUST USING LEMMA\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"STRUCTURE OF XML DATA FILE\"\n",
    "\n",
    "print \"TAG:\", root.tag\n",
    "print \"ATTRIB:\", root.attrib\n",
    "print \n",
    "print \"LEVEL 1 CHILDREN (source documents):\"\n",
    "for l1_child in root[0:5] :\n",
    "    print \"L1 TAG:\", l1_child.tag,\"\\tL1 ATTRIB:\", l1_child.attrib\n",
    "    for l2_child in l1_child [0:5]:\n",
    "        print \"\\tL2 TAG:\", l2_child.tag,\"\\tL2 ATTRIB:\", l2_child.attrib\n",
    "        for l3_child in l2_child [0:5]:\n",
    "            print \"\\t\\tL3 TAG:\", l3_child.tag,\"\\tL3 ATTRIB:\", l3_child.attrib,\"\\tL3 Text:\", l3_child.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"EXAMPLE, LEVEL 2, GET ID:\"\n",
    "# print semcor_root[0][0].attrib['id']\n",
    "print root[0][0].attrib['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print all in order (will freeze up comp if not just a few examples)\n",
    "for text in root[0].iter():\n",
    "    print text.tag, text.attrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#to limit just to instances with lemma 'service' - two ways:\n",
    "for node in root.iter('instance'):\n",
    "    lemma = node.attrib.get('lemma')\n",
    "    pos = node.attrib.get('pos')\n",
    "    ID = node.attrib.get('id')\n",
    "\n",
    "    if lemma == 'service':\n",
    "        print lemma, pos, ID\n",
    "\n",
    "print       \n",
    "\n",
    "for node in root.findall('.//instance'):\n",
    "    lemma = node.attrib.get('lemma')\n",
    "    pos = node.attrib.get('pos')\n",
    "    ID = node.attrib.get('id')\n",
    "    if lemma == 'service':\n",
    "        print lemma, pos, ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"ADDED IN v5\"\"\"\n",
    "\n",
    "#PULLS IN THE ACTUAL TEXT WORD FOR LEMMA - CAN BE USEFUL TO BUILD DICTIONARIES\n",
    "for node in root.findall('.//instance'):\n",
    "    lemma = node.attrib.get('lemma')\n",
    "    pos = node.attrib.get('pos')\n",
    "    ID = node.attrib.get('id')\n",
    "    text = node.text\n",
    "    if lemma == 'work':\n",
    "        print lemma, pos, ID, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make defaultdict for each instanceid\n",
    "from collections import defaultdict\n",
    "instance_dict = defaultdict(dict)\n",
    "\n",
    "for node in root.findall('.//instance'):\n",
    "    lemma = node.attrib.get('lemma')\n",
    "    pos = node.attrib.get('pos')\n",
    "    ID = node.attrib.get('id')\n",
    "    instance_dict['id'] = ID\n",
    "    instance_dict[ID] = {'lemma':lemma, 'pos':pos}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get single instance from created dictionary\n",
    "print instance_dict['d000.s000.t005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#if get error with synsets, uncomment/run the following lines (takes a while)\n",
    "# import nltk\n",
    "# nltk.download()\n",
    "\n",
    "wn.synsets(\"service\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
