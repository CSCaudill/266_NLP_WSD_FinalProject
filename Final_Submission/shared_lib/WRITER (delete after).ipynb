{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile gram_builder.py\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def normalize_counter(c):\n",
    "    \"\"\"Given a dictionary of <item, counts>, return <item, fraction>.\"\"\"\n",
    "    total = sum(c.itervalues())\n",
    "    return {w:float(c[w])/total for w in c}\n",
    "\n",
    "\n",
    "\n",
    "class SimpleTrigramLM(object):\n",
    "    def __init__(self, words):\n",
    "        \"\"\"Build our simple trigram model.\"\"\"\n",
    "        # Raw trigram counts over the corpus. \n",
    "        # c(w | w_1 w_2) = self.counts[(w_2,w_1)][w]\n",
    "        self.counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    \n",
    "        # Iterate through the word stream once.\n",
    "        w_1, w_2 = None, None\n",
    "        for word in words:\n",
    "            if w_1 is not None and w_2 is not None:\n",
    "                # Increment trigram count.\n",
    "                self.counts[(w_2,w_1)][word] += 1\n",
    "            # Shift context along the stream of words.\n",
    "            w_2 = w_1\n",
    "            w_1 = word\n",
    "            \n",
    "        # Normalize so that for each context we have a valid probability\n",
    "        # distribution (i.e. adds up to 1.0) of possible next tokens.\n",
    "        self.probas = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "        for context, ctr in self.counts.iteritems():\n",
    "            self.probas[context] = normalize_counter(ctr)\n",
    "            \n",
    "    def next_word_proba(self, word, seq):\n",
    "        \"\"\"Compute p(word | seq)\"\"\"\n",
    "        context = tuple(seq[-2:])  # last two words\n",
    "        return self.probas[context].get(word, 0.0)\n",
    "    \n",
    "    def predict_next(self, seq):\n",
    "        \"\"\"Sample a word from the conditional distribution.\"\"\"\n",
    "        context = tuple(seq[-2:])  # last two words\n",
    "        pc = self.probas[context]  # conditional distribution\n",
    "        words, probs = zip(*pc.iteritems())  # convert to list\n",
    "        return np.random.choice(words, p=probs)\n",
    "    \n",
    "    def score_seq(self, seq, verbose=False):\n",
    "        \"\"\"Compute log probability (base 2) of the given sequence.\"\"\"\n",
    "        score = 0.0\n",
    "        count = 0\n",
    "        # Start at third word, since we need a full context.\n",
    "        for i in range(2, len(seq)):\n",
    "            if (seq[i] == \"<s>\" or seq[i] == \"</s>\"):\n",
    "                continue  # Don't count special tokens in score.\n",
    "            s = np.log2(self.next_word_proba(seq[i], seq[i-2:i]))\n",
    "            score += s\n",
    "            count += 1\n",
    "            # DEBUG\n",
    "            if verbose:\n",
    "                print \"log P(%s | %s) = %.03f\" % (seq[i], \" \".join(seq[i-2:i]), s)\n",
    "        return score, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import re\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# For pretty-printing\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import jinja2\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flatten a list-of-lists into a single list.\"\"\"\n",
    "    return list(itertools.chain.from_iterable(list_of_lists))\n",
    "\n",
    "HIGHLIGHT_BUTTON_TMPL = jinja2.Template(\"\"\"\n",
    "<script>\n",
    "colors_on = true;\n",
    "function color_cells() {\n",
    "  var ffunc = function(i,e) {return e.innerText {{ filter_cond }}; }\n",
    "  var cells = $('table.dataframe').children('tbody')\n",
    "                                  .children('tr')\n",
    "                                  .children('td')\n",
    "                                  .filter(ffunc);\n",
    "  if (colors_on) {\n",
    "    cells.css('background', 'white');\n",
    "  } else {\n",
    "    cells.css('background', '{{ highlight_color }}');\n",
    "  }\n",
    "  colors_on = !colors_on;\n",
    "}\n",
    "$( document ).ready(color_cells);\n",
    "</script>\n",
    "<form action=\"javascript:color_cells()\">\n",
    "<input type=\"submit\" value=\"Toggle highlighting (val {{ filter_cond }})\"></form>\n",
    "\"\"\")\n",
    "\n",
    "RESIZE_CELLS_TMPL = jinja2.Template(\"\"\"\n",
    "<script>\n",
    "var df = $('table.dataframe');\n",
    "var cells = df.children('tbody').children('tr')\n",
    "                                .children('td');\n",
    "cells.css(\"width\", \"{{ w }}px\").css(\"height\", \"{{ h }}px\");\n",
    "</script>\n",
    "\"\"\")\n",
    "\n",
    "def render_matrix(M, rows=None, cols=None, dtype=float,\n",
    "                        min_size=30, highlight=\"\"):\n",
    "    html = [pd.DataFrame(M, index=rows, columns=cols,\n",
    "                         dtype=dtype)._repr_html_()]\n",
    "    if min_size > 0:\n",
    "        html.append(RESIZE_CELLS_TMPL.render(w=min_size, h=min_size))\n",
    "\n",
    "    if highlight:\n",
    "        html.append(HIGHLIGHT_BUTTON_TMPL.render(filter_cond=highlight,\n",
    "                                             highlight_color=\"yellow\"))\n",
    "\n",
    "    return \"\\n\".join(html)\n",
    "    \n",
    "def pretty_print_matrix(*args, **kwargs):\n",
    "    \"\"\"Pretty-print a matrix using Pandas.\n",
    "\n",
    "    Optionally supports a highlight button, which is a very, very experimental\n",
    "    piece of messy JavaScript. It seems to work for demonstration purposes.\n",
    "\n",
    "    Args:\n",
    "      M : 2D numpy array\n",
    "      rows : list of row labels\n",
    "      cols : list of column labels\n",
    "      dtype : data type (float or int)\n",
    "      min_size : minimum cell size, in pixels\n",
    "      highlight (string): if non-empty, interpreted as a predicate on cell\n",
    "      values, and will render a \"Toggle highlighting\" button.\n",
    "    \"\"\"\n",
    "    html = render_matrix(*args, **kwargs)\n",
    "    display(HTML(html))\n",
    "\n",
    "\n",
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)\n",
    "\n",
    "\n",
    "##\n",
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset): return word\n",
    "    else: return \"<unk>\" # unknown token\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]\n",
    "\n",
    "##\n",
    "# Data loading functions\n",
    "import nltk\n",
    "import vocabulary\n",
    "\n",
    "def get_corpus(name=\"brown\"):\n",
    "    return nltk.corpus.__getattr__(name)\n",
    "\n",
    "def sents_to_tokens(sents, vocab):\n",
    "    \"\"\"Returns an flattened list of the words in the sentences, with normal padding.\"\"\"\n",
    "    padded_sentences = ([\"<s>\"] + s + [\"</s>\"] for s in sents)\n",
    "    # This will canonicalize words, and replace anything not in vocab with <unk>\n",
    "    return np.array([canonicalize_word(w, wordset=vocab.wordset)\n",
    "                     for w in flatten(padded_sentences)], dtype=object)\n",
    "\n",
    "def build_vocab(corpus, V=10000):\n",
    "    token_feed = (canonicalize_word(w) for w in corpus.words())\n",
    "    vocab = vocabulary.Vocabulary(token_feed, size=V)\n",
    "    return vocab\n",
    "\n",
    "def get_train_test_sents(corpus, split=0.8, shuffle=True):\n",
    "    \"\"\"Get train and test sentences.\n",
    "\n",
    "    Args:\n",
    "      corpus: nltk.corpus that supports sents() function\n",
    "      split (double): fraction to use as training set\n",
    "      shuffle (int or bool): seed for shuffle of input data, or False to just\n",
    "      take the training data as the first xx% contiguously.\n",
    "\n",
    "    Returns:\n",
    "      train_sentences, test_sentences ( list(list(string)) ): the train and test\n",
    "      splits\n",
    "    \"\"\"\n",
    "    sentences = np.array(corpus.sents(), dtype=object)\n",
    "    fmt = (len(sentences), sum(map(len, sentences)))\n",
    "    print \"Loaded %d sentences (%g tokens)\" % fmt\n",
    "\n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(shuffle)\n",
    "        rng.shuffle(sentences)  # in-place\n",
    "    train_frac = 0.8\n",
    "    split_idx = int(train_frac * len(sentences))\n",
    "    train_sentences = sentences[:split_idx]\n",
    "    test_sentences = sentences[split_idx:]\n",
    "\n",
    "    fmt = (len(train_sentences), sum(map(len, train_sentences)))\n",
    "    print \"Training set: %d sentences (%d tokens)\" % fmt\n",
    "    fmt = (len(test_sentences), sum(map(len, test_sentences)))\n",
    "    print \"Test set: %d sentences (%d tokens)\" % fmt\n",
    "\n",
    "    return train_sentences, test_sentences\n",
    "\n",
    "def preprocess_sentences(sentences, vocab):\n",
    "    \"\"\"Preprocess sentences by canonicalizing and mapping to ids.\n",
    "\n",
    "    Args:\n",
    "      sentences ( list(list(string)) ): input sentences\n",
    "      vocab: Vocabulary object, already initialized\n",
    "\n",
    "    Returns:\n",
    "      ids ( array(int) ): flattened array of sentences, including boundary <s>\n",
    "      tokens.\n",
    "    \"\"\"\n",
    "    # Add sentence boundaries, canonicalize, and handle unknowns\n",
    "    words = flatten([\"<s>\"] + s + [\"</s>\"] for s in sentences)\n",
    "    words = [canonicalize_word(w, wordset=vocab.word_to_id)\n",
    "             for w in words]\n",
    "    return np.array(vocab.words_to_ids(words))\n",
    "\n",
    "##\n",
    "# Use this function\n",
    "def load_corpus(name, split=0.8, V=10000, shuffle=0):\n",
    "    \"\"\"Load a named corpus and split train/test along sentences.\"\"\"\n",
    "    corpus = get_corpus(name)\n",
    "    vocab = build_vocab(corpus, V)\n",
    "    train_sentences, test_sentences = get_train_test_sents(corpus, split, shuffle)\n",
    "    train_ids = preprocess_sentences(train_sentences, vocab)\n",
    "    test_ids = preprocess_sentences(test_sentences, vocab)\n",
    "    return vocab, train_ids, test_ids\n",
    "\n",
    "##\n",
    "# Use this function\n",
    "def batch_generator(ids, batch_size, max_time):\n",
    "    \"\"\"Convert ids to data-matrix form.\"\"\"\n",
    "    # Clip to multiple of max_time for convenience\n",
    "    clip_len = ((len(ids)-1) / batch_size) * batch_size\n",
    "    input_w = ids[:clip_len]     # current word\n",
    "    target_y = ids[1:clip_len+1]  # next word\n",
    "    # Reshape so we can select columns\n",
    "    input_w = input_w.reshape([batch_size,-1])\n",
    "    target_y = target_y.reshape([batch_size,-1])\n",
    "\n",
    "    # Yield batches\n",
    "    for i in xrange(0, input_w.shape[1], max_time):\n",
    "\tyield input_w[:,i:i+max_time], target_y[:,i:i+max_time]\n",
    "\n",
    "##########~~~~~~~~~~~~~~~ STUDENT ADDED ~~~~~~~~~~~~##############\n",
    "\n",
    "import ws_encoder\n",
    "\n",
    "\n",
    "def canonicalize_word_senses_train(word, wordset=None, digits=True):\n",
    "    #SIMPLIFIED  because <unk> was coming up in training for word senses like 2%1:23:00::, throwing off training.\n",
    "    word = word.lower()\n",
    "    return word\n",
    "\n",
    "def preprocess_word_senses(word_senses, ws_vocab):\n",
    "    \"\"\"Preprocess sentences by canonicalizing and mapping to ids.\n",
    "\n",
    "    Args:\n",
    "      sentences ( list(list(string)) ): input sentences\n",
    "      vocab: Vocabulary object, already initialized\n",
    "\n",
    "    Returns:\n",
    "      ids ( array(int) ): flattened array of sentences, including boundary <s>\n",
    "      tokens.\n",
    "    \"\"\"\n",
    "    # Add sentence boundaries, canonicalize, and handle unknowns\n",
    "    words = [canonicalize_word(w, wordset=ws_vocab.word_sense_to_id)\n",
    "             for w in word_senses]\n",
    "    return np.array(ws_vocab.word_senses_to_ids(words))\n",
    "\n",
    "def build_ws_encodings(word_senses, V=None):\n",
    "    token_feed = (canonicalize_word_senses_train(ws) for ws in word_senses)\n",
    "    ws_encodings = ws_encoder.WS_Encoder(token_feed, size=V)\n",
    "    return ws_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ws_encoder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ws_encoder.py\n",
    "\n",
    "import collections\n",
    "\n",
    "class WS_Encoder(object):\n",
    "    \n",
    "  \"\"\"SIGNIFICANTLY LEVERAGED VOCABULARY.PY FROM ASSIGNMENT 3\"\"\"\n",
    "\n",
    "#   START_TOKEN = \"<s>\"\n",
    "#   END_TOKEN = \"</s>\"\n",
    "  UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    # leave space for \"<s>\", \"</s>\", and \"<unk>\" - MODIFYING\n",
    "    #leave space now just for \"<unk>\"\n",
    "    \n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 1))\n",
    "    ws_vocab = ([self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "    \n",
    "    ### NOTE: is actually ID to WORD SENSE, but left as ID TO WORD to allow for easier naming between files\n",
    "    \n",
    "    # Assign an id to each word, by frequency\n",
    "    self.id_to_word_sense = dict(enumerate(ws_vocab))\n",
    "    self.word_sense_to_id = {v:k for k,v in self.id_to_word_sense.iteritems()}\n",
    "    self.size = len(self.id_to_word_sense)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience\n",
    "    self.wordset = set(self.word_sense_to_id.iterkeys())\n",
    "\n",
    "    # Store special IDs\n",
    "#     self.START_ID = self.word_to_id[self.START_TOKEN]\n",
    "#     self.END_ID = self.word_to_id[self.END_TOKEN]\n",
    "    self.UNK_ID = self.word_sense_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  def word_senses_to_ids(self, words):\n",
    "    return [self.word_sense_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_word_senses(self, ids):\n",
    "    return [self.id_to_word_senses[i] for i in ids]\n",
    "\n",
    "#   def sentence_to_ids(self, words):\n",
    "#     return [self.START_ID] + self.words_to_ids(words) + [self.END_ID]\n",
    "\n",
    "  def ordered_word_senses(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_word_senses(range(self.size))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
